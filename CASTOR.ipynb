{"cells":[{"cell_type":"markdown","metadata":{"id":"GoiUwFhRD-5i"},"source":["# **CASTOR $\\iota$**"]},{"cell_type":"markdown","metadata":{"id":"NHd0Ji_0YSZv"},"source":["Core collApse Spectral Templates generatOR (CASTOR) is a novel software for data analysis, developed in 2023 by Andrea Simongini, with the supervision of Dr. Fabio Ragosta, Dr. Silvia Piranomonte and Dr. Irene Di Palma. The aim of the software is to build database of synthetic spectra for core collapse supernovae (CCSNe) and to reconstruct the parametric maps for them.\n","\n","CASTOR collects Photometric (LCs) and Spectrometric (SPs) data for a catalogue of 67 CCSNe. The light curves are taken from Open Supernova Catalog, while the spectra are taken from WISeREP.\n","\n","The software is laid as follows: first it statistically analyze the training set, determining the calibration parameters and the wavelength interval into which to build templates; then, it compares the new supernova LCs with them of the training set, determining the reference supernova; furthermore, it builds 100 spectral templates (from the time of explosion to 200+ days) using LCs of the new supernova and SPs of the reference ones. Finally, using the new templates, it estimates the parameters of the new SN. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61858,"status":"ok","timestamp":1703672589235,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"f8iB0eKT1H32","outputId":"fb6134f8-16c8-4426-b81d-5d5eb071b5a2"},"outputs":[],"source":["'''\n","In this cell I import every external library\n","I'll use throughout the code\n","'''\n","\n","import  numpy as np\n","import  matplotlib.pyplot as plt\n","import  pandas as pd\n","import  math\n","import  os\n","import  george\n","import  warnings\n","import  openpyxl\n","import  urllib\n","import  astropy\n","import  time\n","import  random\n","import  itertools\n","import  json\n","import  sys\n","from    urllib.request  import urlopen\n","from    astropy.io      import ascii\n","from    collections     import defaultdict\n","from    itertools       import chain\n","from    functools       import partial\n","from    collections     import Counter\n","from    numpy           import nanmedian, NaN\n","from    scipy.optimize  import curve_fit\n","from    scipy.stats     import chisquare\n","from    scipy.integrate import trapezoid  as trapz\n","from    astropy.time    import Time       as TT\n","from    scipy           import optimize   as op\n","import  george\n","from    george.kernels  import Matern32Kernel"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1703672589236,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"l7mrwOgjcNMh"},"outputs":[],"source":["'''\n","In this cell I declare some\n","functions\n","'''\n","\n","def read_csv(path, file_name, sheet, column_name, list_):\n","\n","  file_location = path + file_name\n","  table = pd.read_excel(io=file_location, sheet_name=sheet)\n","  array = []\n","\n","  if list_ == True:\n","    for i in range(len(table)):\n","      array.append(str(table.loc[:,column_name][i]))\n","  else:\n","    for i in range(len(table)):\n","      array = np.array(table.loc[:, column_name].astype(float))\n","\n","  return array\n","\n","def get_color_band(band_name):\n","  color_band = {\n","      'UVW2': 'purple',\n","      'UVM2': 'darkviolet',\n","      'UVW1': 'blueviolet',\n","      \"u'\": 'indigo',\n","      'u': 'royalblue',\n","      'U': 'dodgerblue',\n","      'B': 'blue',\n","      'g': 'limegreen',\n","      \"g'\": 'forestgreen',\n","      'V': 'green',\n","      'r': 'firebrick',\n","      \"r'\": 'darkred',\n","      'R': 'red',\n","      'i': 'lightcoral',\n","      \"i'\": 'coral',\n","      'I': 'orange',\n","      'z': 'darkgoldenrod',\n","      \"z'\": 'goldenrod',\n","      'Y': 'chocolate',\n","      'J': 'saddlebrown',\n","      'H': 'darkred',\n","      'K': 'maroon',\n","      'Ks': 'darkred',\n","  }\n","\n","  if band_name in color_band.keys():\n","      return color_band[band_name]\n","  else:\n","      return 'grey'\n","\n","def nll(p, y, gp, computed):\n","  gp.set_parameter_vector(p)\n","  ll = gp.log_likelihood(y, quiet=True)\n","  return -ll if np.isfinite(ll) else 1e25\n","\n","\n","def grad_nll(p, y, gp, computed):\n","  gp.set_parameter_vector(p)\n","  return -gp.grad_log_likelihood(y, quiet=True)\n","\n","def new_mean(array):\n","  if len(array) >0:\n","    media = round(np.mean(array))\n","  else:\n","    media = 0\n","  return media\n","\n","def blackbody(wave, temperature, radius, zeta, distance):\n","\n","  h          =  planck_ergs      #erg/s\n","  c          =  light_vel_A      #A/s\n","  k          =  boltz_ergK       #erg/K\n","\n","  BB = ( 2 * h * c**2 ) / wave**5 * ( 1 / (np.exp(h * c / (wave * k * temperature)) -1 )) * 10e+16  #erg/s/cm^2/A\n","  F  = ( zeta * radius / distance ) ** 2 * np.pi * BB\n","  F  = F * wave                  #erg/s/cm^2\n","\n","  return F\n","\n","def exponential(x, a, b, c):\n","\n","  return a * np.exp(b * x) + c\n","\n","\n","def linear_fun(x, m, q):\n","  return m*x + q\n","\n","def linear_fit(x, y, err):\n","\n","  par, _ = curve_fit(linear_fun, x, y, sigma=err)\n","  m = par[0]\n","  q = par[1]\n","  return m, q"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1703672589236,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"Q98sS0kKyfF0"},"outputs":[],"source":["def sampling_step(x):\n","  x  = np.sort(x)\n","  dx = []\n","  for i in range(len(x) - 1):\n","    if x[i+1] != x[i]:\n","      dx.append(x[i+1] - x[i])\n","\n","  return dx\n","\n","def magnitude_order(num):\n","    if num == 0:\n","        return 0\n","\n","    absnum = abs(num)\n","    order = math.log10(absnum)\n","    res = math.floor(order)\n","\n","    return res\n","\n","def avg_distance(x):\n","  aux = []\n","  for i in range(len(x)-1):\n","    aux.append(x[i+1]-x[i])\n","  return abs(np.mean(aux))\n","\n","def big_step(x):\n","  aux = []\n","  for i in range(len(x)-1):\n","    aux.append(x[i+1]-x[i])\n","\n","  big_step = np.max(aux)\n","  for i in range(len(x)-1):\n","    if x[i+1] - x[i] == big_step:\n","      minor = x[i]\n","      major = x[i+1]\n","\n","  return minor, major\n","\n","\n","def time_sorting(array1, array2):\n","\n","  zip_list   = zip(array1, array2)\n","  sort_list  = sorted(zip_list, reverse=False)  \n","  unzip_list = zip(*sort_list)\n","\n","  new_array1, new_array2 = map(list, unzip_list)\n","\n","  return np.array(new_array1), np.array(new_array2)\n","\n","\n","def order_sorting(array1, array2, array3):\n","  sort_indices = np.argsort(array1)\n","  new_array1 = array1[sort_indices]\n","  new_array2 = array2[sort_indices]\n","  new_array3 = array3[sort_indices]\n","  return new_array1, new_array2, new_array3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1703672589237,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"sDn3WatSjSrF"},"outputs":[],"source":["'''\n","Define the paths.\n","'''\n","\n","file_path          = os.path.abspath('Castor.ipynb')\n","folder_path        = os.path.dirname(file_path)\n","LOCALPATH          = folder_path + \"/\"\n","\n","excel_path         =  LOCALPATH + \"Reference\"   + \"/\"\n","results_path       =  LOCALPATH + \"Results\"     + \"/\"\n","templates_path     =  LOCALPATH + \"Templates\"   + \"/\"\n","app_path           =  LOCALPATH + \"Application\" + \"/\"\n","data_path          =  LOCALPATH + \"Data\"        + \"/\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1703672589237,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"J5NmTimcluPT"},"outputs":[],"source":["'''\n","Define some important parameters:\n","acceptance : defined to count the spectrometric filter\n","treshold   : when there's more it's okay\n","nsteps     : how many steps for MHMC ?\n","'''\n","\n","acc           = 0.8  # input(\"Inserire valore di accettazione (consigliato 0.8): \")\n","tresh         = 3    # input(\"Inserire valore di soglia (minimo 3): \")\n","nsteps        = 5000 # input(\"Inserire valore di passi (consigliato 5000): \")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2427,"status":"ok","timestamp":1703672591652,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"KbSkjvRXmFnY"},"outputs":[],"source":["'''\n","Define some important parameters:\n","acceptance : I count only when there are more than 3 light curves\n","treshold   : defined to count the spectrometric filter\n","'''\n","\n","bandpass               = {}\n","bandpass['filterlist'] = filterlist = read_csv(excel_path, 'bandpass.xlsx', 'Complete', \"Filter\",     True)\n","bandpass['centroid']   = centroid   = read_csv(excel_path, 'bandpass.xlsx', 'Complete', \"Wavelength\", False)\n","bandpass['fwhm']       = fwhm       = read_csv(excel_path, 'bandpass.xlsx', 'Complete', \"FWHM\",       False)\n","bandpass['corr_mag']   = corr_mag   = read_csv(excel_path, 'bandpass.xlsx', 'Complete', \"mAB-mV\",     False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703672591653,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"-mf6CVJPANiS"},"outputs":[],"source":["'''\n","Here I declare some absorption lines common in CCSNe\n","The letters added to Helium and Iron lines are arbitrarily \n","placed in order to distinguish between different wavelengths. \n","'''\n","\n","lines = {'He Ia' : 4471 ,\n","         'He Ib' : 5876 ,\n","         'He Ic' : 6678 ,\n","         'He Id' : 7065 ,\n","         'He Ie' : 10830,\n","         'Fe IIa': 4900 ,\n","         'Fe IIb': 5500 ,\n","         'Fe IIc': 5300 ,\n","         '[Fe II]': 7155,\n","         'Ha'    : 6563 ,\n","         'Hb'    : 4861 ,\n","         'Hg'    : 4341 ,\n","         'Hd'    : 4102 ,\n","         '[Si II]': 6150}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703672591653,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"pQXPHo5VjH3W"},"outputs":[],"source":["'''\n","Here I define some constants\n","'''\n","\n","light_vel_A = 3e+18         #A/s\n","light_vel_km= 3e+5          #km/s\n","planck_ergs = 6.63e-27      #erg/s\n","boltz_ergK  = 1.38e-16      #erg/K\n","boltz_eVK   = 8.61327e-5    #eV/K\n","R_sun_mpc   = 2.25e-14      #Mpc\n","R_sun_cm    = 6.960e+10     #cm\n","R_sun_km    = 6.960e+5      #km\n","\n","Hubble_70   = 70            #km/s/Mpc\n","\n","M_sun_g     = 1.98892e+33   #g\n","\n","L_sun_ergs  = 3.832e+33"]},{"cell_type":"markdown","metadata":{"id":"8R-8O1XBj94T"},"source":["# Section 0: data extraction and results saving\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":320,"status":"ok","timestamp":1703672591968,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"04mG-rqom7MY"},"outputs":[],"source":["SN_names      = read_csv(excel_path, 'SN_names.xlsx', 'SN_names',\"Name\",True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1703672591969,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"t4EKH-UjlcRa"},"outputs":[],"source":["class data_extraction:\n","\n","  def __init__(self, name, path, acc, tresh, bandpass):\n","\n","    self.name       = name\n","    self.path       = path\n","    self.acc        = acc\n","    self.tresh      = tresh\n","    self.filterlist = bandpass['filterlist']\n","    self.centroid   = bandpass['centroid']\n","    self.fwhm       = bandpass['fwhm']\n","\n","\n","  def collect_lightcurves(self):\n","    \n","    lc_path     = self.path + 'data_lightcurves' + \"/\" + self.name + \".dat\"\n","    lc_set      = defaultdict(int)\n","    time_array  = ([])\n","    mag_array   = ([])\n","    emag_array  = ([])\n","    band_list   = []\n","\n","    with open(lc_path, 'r') as file:\n","        for line in file:\n","            parts = line.strip().split()\n","            time = float(parts[0])\n","            mag  = float(parts[1])\n","            emag = float(parts[2])\n","            band = parts[3]\n","\n","            time_array = np.append(time_array, time)\n","            mag_array  = np.append(mag_array, mag)\n","            emag_array = np.append(emag_array, emag)\n","            band_list.append(band)\n","\n","    lc_filters = [band_list[0]]\n","    for i in range(len(band_list)-1):\n","      if band_list[i] != band_list[i+1]:\n","        lc_filters.append(band_list[i+1])\n","\n","    for filtro in lc_filters:\n","      lc_set['time_%s'%filtro]  = [time_array[index]   for index, val in enumerate(band_list) if val == filtro]\n","      lc_set['mag_%s' %filtro]  = [mag_array[index]    for index, val in enumerate(band_list) if val == filtro]\n","      lc_set['emag_%s'%filtro]  = [emag_array[index]   for index, val in enumerate(band_list) if val == filtro]\n","\n","    return lc_filters, lc_set\n","\n","\n","  def collect_spectra(self):\n","    \n","    sp_set        = defaultdict(int)\n","    spectra_path  = self.path + 'data_spectra' + \"/\" + self.name + \"/\"\n","    all_files     = os.listdir(spectra_path)\n","    valid_files   = [file for file in all_files if not (file.startswith('.') or file.lower() == 'desktop.ini')]\n","    all_epochs    = ([])\n","\n","\n","    for single_file in valid_files:\n","      file_name    = single_file.split('_')[1]\n","      epoch        = file_name.rsplit('.', 1)[0]\n","\n","      all_epochs  = np.append(all_epochs, float(epoch))\n","      data        = ascii.read(spectra_path + single_file)\n","      sp_set['wave_%s'%str(epoch)]  = data[0][:]\n","      sp_set['ewave_%s'%str(epoch)] = data[1][:]\n","      sp_set['flux_%s'%str(epoch)]  = data[2][:]\n","      sp_set['eflux_%s'%str(epoch)] = data[3][:]\n","\n","    return all_epochs, sp_set\n","\n","\n","  def spectral_bands(self, sp_set, files):\n","    \n","    sp_filters = {}\n","\n","    for epoch in files:\n","\n","      sp_filters['filt_%s'%str(epoch)] = []\n","      wave    = np.array(sp_set['wave_' + str(epoch)])\n","      \n","      for filtro in self.filterlist:\n","\n","        min_wave  = [self.centroid[i]-self.fwhm[i]/2   for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","        max_wave  = [self.centroid[i]+self.fwhm[i]/2   for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","        ff        = [self.fwhm[i]                      for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","\n","        mini      = min_wave[0] + ff[0] * self.acc\n","        maxi      = max_wave[0] - ff[0] * self.acc\n","\n","        mask  = [val for val in wave if min_wave[0] <= val <= mini]\n","        mask1 = [val for val in wave if maxi <= val <= max_wave[0]]\n","\n","        if mask and mask1:\n","          sp_filters['filt_%s'%str(epoch)].append(filtro)\n","\n","    return sp_filters\n","\n","\n","  def matched_bands(self, lc_filters, sp_filters):\n","\n","    total_filters = sum(sp_filters.values(), [] )\n","\n","    return sorted(set(lc_filters).intersection(total_filters))\n","\n","\n","  def total_exploration(self):\n","    \n","    lc_filters, lc_set  = self.collect_lightcurves()\n","    files, sp_set       = self.collect_spectra()\n","    sp_filters          = self.spectral_bands(sp_set, files)\n","    matched             = self.matched_bands(lc_filters, sp_filters)\n","\n","    return (files, sp_set, lc_filters, lc_set, sp_filters, matched)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":356,"status":"ok","timestamp":1703672592312,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"rYW4lRrdZyV1"},"outputs":[],"source":["class templates_extraction:\n","\n","  def __init__(self, name, path, acc, tresh, bandpass):\n","\n","    self.name       = name\n","    self.path       = path\n","    self.acc        = acc\n","    self.tresh      = tresh\n","    self.filterlist = bandpass['filterlist']\n","    self.centroid   = bandpass['centroid']\n","    self.fwhm       = bandpass['fwhm']\n","\n","\n","  def read_templates(self):\n","\n","    temp_set      = defaultdict(int)\n","    spectra_path  = self.path + 'Templates' + \"/\"\n","    all_files     = os.listdir(spectra_path)\n","    valid_files   = [file for file in all_files if not (file.startswith('.') or file.lower() == 'desktop.ini')]\n","    all_epochs    = ([])\n","\n","    for single_file in valid_files:\n","      file_name    = single_file.split('_')[1]\n","      epoch        = float(file_name.rsplit('.', 1)[0])\n","      all_epochs   = np.append(all_epochs, float(epoch))\n","      data         = ascii.read(spectra_path + single_file)\n","\n","      temp_set['wave_%s'%str(epoch)]  = np.array(data[0][:])\n","      temp_set['flux_%s'%str(epoch)]  = np.array(data[1][:])\n","      temp_set['eflux_%s'%str(epoch)] = np.array(data[2][:])\n","\n","    return all_epochs, temp_set\n","\n","\n","  def temp_filters(self, all_epochs, temp_set):\n","    \n","    sp_filters = defaultdict(int)\n","\n","    for epoch in all_epochs:\n","\n","      sp_filters['filt_%s'%str(epoch)] = []\n","      wave    = np.array(temp_set['wave_' + str(epoch)])\n","\n","      for filtro in self.filterlist:\n","\n","        min_wave  = [self.centroid[i]-self.fwhm[i]/2   for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","        max_wave  = [self.centroid[i]+self.fwhm[i]/2   for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","        ff        = [self.fwhm[i]                      for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","\n","        mini      = min_wave[0] + ff[0] * self.acc\n","        maxi      = max_wave[0] - ff[0] * self.acc\n","\n","        mask  = [val for val in wave if min_wave[0] <= val <= mini]\n","        mask1 = [val for val in wave if maxi <= val <= max_wave[0]]\n","\n","        if mask and mask1:\n","          sp_filters['filt_%s'%str(epoch)].append(filtro)\n","\n","    return sp_filters\n","\n","\n","  def use_templates(self):\n","\n","    files, temp_set = self.read_templates()\n","    sp_filters      = self.temp_filters(files, temp_set)\n","\n","    return files, temp_set, sp_filters"]},{"cell_type":"markdown","metadata":{"id":"WBkNOVySkHp0"},"source":["# Section 1: preliminary studies"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1703672592312,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"Y35mVjZhnMBV"},"outputs":[],"source":["class calibration():\n","\n","  def __init__(self, name, files, sp_set, lc_filters, lc_set, matched, bandpass):\n","\n","    self.name         = name\n","    self.files        = files\n","    self.sp_set       = sp_set\n","    self.lc_filters   = lc_filters\n","    self.lc_set       = lc_set\n","    self.matched      = matched\n","    self.filterlist   = bandpass['filterlist']\n","    self.centroid     = bandpass['centroid']\n","    self.fwhm         = bandpass['fwhm']\n","    self.corr_mag     = bandpass['corr_mag']\n","\n","  def gaussian_process(self):\n","    \n","    gp_set = defaultdict(int)\n","    for filtro in self.lc_filters:\n","      x           = np.array(self.lc_set['time_' + filtro])\n","      y           = np.array(self.lc_set['mag_'  + filtro])\n","      yerr        = np.array(self.lc_set['emag_' + filtro])\n","      amplitude   = np.mean(y)\n","\n","      lengthscale0 = np.mean(x)\n","      lengthscale1 = np.min(sampling_step(x))\n","      lengthscale2 = np.max(sampling_step(x))\n","\n","      k0           = amplitude * Matern32Kernel(lengthscale0)\n","      k1           = amplitude * Matern32Kernel(lengthscale1)\n","      k2           = amplitude * Matern32Kernel(lengthscale2)\n","\n","      kernel       = k1 + k2 + k0\n","\n","      gp          = george.GP(kernel)\n","      star        = gp.compute(x, yerr)\n","\n","      p0          = gp.get_parameter_vector()\n","      results     = op.minimize(nll, p0, args = (y, gp, star), jac=grad_nll, method=\"L-BFGS-B\")\n","      gp.set_parameter_vector(results.x)\n","\n","      t           = np.linspace(np.min(x), np.max(x), 5000)\n","      mu, cov     = gp.predict(y, t)\n","      std         = np.sqrt(cov.diagonal())\n","\n","      gp_set['t_%s'%filtro]    = t\n","      gp_set['mu_%s'%filtro]   = mu\n","      gp_set['std_%s'%filtro]  = std\n","\n","    return gp_set\n","\n","  def mangling(self, gp_set, cut = True):\n","    \n","    SP        = ([])\n","    LC        = ([])\n","    ERR       = ([])\n","\n","    for epoch in self.files:\n","\n","      wave    = np.array(self.sp_set['wave_' + str(epoch)])\n","      flux    = np.array(self.sp_set['flux_' + str(epoch)])\n","      f_nu    = flux * wave**2/ light_vel_A\n","\n","      for filtro in self.matched:\n","        t           = np.array(gp_set['t_'    + filtro])\n","        mu          = np.array(gp_set['mu_'   + filtro])\n","        std         = np.array(gp_set['std_'  + filtro])\n","\n","        closest = min(t, key = lambda x: abs(x-epoch))\n","        mg_lc   = [mu[i]  for i in range(0, len(mu))   if t[i] == closest]\n","        err_lc  = [std[i] for i in range(0, len(std))  if t[i] == closest]\n","\n","        mg_lc   = mg_lc[0]\n","        err_lc  = err_lc[0]\n","\n","        mean_flux = np.mean(f_nu)\n","\n","        if mean_flux < 0 :\n","          mean_flux = -np.inf\n","\n","        mg_sp = -2.5*np.log10(mean_flux) - 48.6\n","\n","        cor_mag = [self.corr_mag[i] for i in range(len(self.corr_mag)) if filtro == self.filterlist[i]]\n","        if math.isnan(cor_mag[0]) == False:\n","          mg_sp = mg_sp - cor_mag[0]\n","\n","        if math.isfinite(mg_sp):\n","          SP  = np.append(SP, mg_sp)\n","          LC  = np.append(LC, mg_lc)\n","          ERR = np.append(ERR, err_lc)\n","\n","        if cut == True:\n","          save_sp   = np.array([])\n","          save_lc   = np.array([])\n","          save_err  = np.array([])\n","          for i in range(len(ERR)):\n","            if ERR[i] < 10*np.mean(ERR):\n","              if abs(SP[i]-LC[i]) < 2:\n","                save_sp   = np.append(save_sp, SP[i])\n","                save_lc   = np.append(save_lc, LC[i])\n","                save_err  = np.append(save_err, ERR[i])\n","          SP  = save_sp\n","          LC  = save_lc\n","          ERR = save_err\n","\n","    return  SP, LC, ERR\n","\n","\n","  def remangling(self, gp_set, SP, LC, ERR, cut = True ):\n","\n","    m0, q0 = linear_fit(SP, LC, ERR)\n","\n","    for epoch in self.files:\n","      wave    = np.array(self.sp_set['wave_' + str(epoch)])\n","      flux    = np.array(self.sp_set['flux_' + str(epoch)])\n","      extraSP = ([])\n","      extraLC = ([])\n","      extraER = ([])\n","\n","      out_filters = list(set(self.lc_filters) - set(self.matched))\n","\n","      for filtro in out_filters:\n","        t           = np.array(gp_set['t_'    + filtro])\n","        mu          = np.array(gp_set['mu_'   + filtro])\n","        std         = np.array(gp_set['std_'  + filtro])\n","\n","        closest     = min(t, key = lambda x: abs(x-epoch))\n","        mg_lc       = [mu[i]  for i in range(0, len(mu)) if t[i] == closest]\n","        mg_lc       = mg_lc[0]\n","\n","        eff_wav     = [self.centroid[i] for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","        cor_mag     = [self.corr_mag[i] for i in range(len(self.corr_mag))    if self.filterlist[i] == filtro]\n","\n","        if math.isnan(cor_mag[0]) == False: cor_mag = cor_mag[0]\n","        else: cor_mag = 0\n","\n","        mg_sp       = (mg_lc - q0) / m0\n","        new_flux    = light_vel_A * 10**(- 0.4* (mg_sp + 48.6 - cor_mag)) / eff_wav[0]**2\n","        flux        = np.append(flux, new_flux)\n","        wave        = np.append(wave, eff_wav[0])\n","        wave, flux  = time_sorting(wave, flux)\n","\n","      f_nu = flux * wave**2 / light_vel_A\n","\n","      for filtro in out_filters:\n","\n","        t           = np.array(gp_set['t_'    + filtro])\n","        mu          = np.array(gp_set['mu_'   + filtro])\n","        std         = np.array(gp_set['std_'  + filtro])\n","\n","        closest = min(t, key = lambda x: abs(x-epoch))\n","        mg_lc   = [mu[i]  for i in range(0, len(mu))   if t[i] == closest]\n","        err_lc  = [std[i] for i in range(0, len(std))  if t[i] == closest]\n","        mg_lc   = mg_lc[0]\n","        err_lc  = err_lc[0]\n","\n","        min_wave = [self.centroid[i]-self.fwhm[i]/2   for i in range(0, len(self.filterlist))  if self.filterlist[i] == filtro]\n","        max_wave = [self.centroid[i]+self.fwhm[i]/2   for i in range(0, len(self.filterlist))  if self.filterlist[i] == filtro]\n","        ff       = [self.fwhm[i]                      for i in range(0, len(self.filterlist))  if self.filterlist[i] == filtro]\n","        ff       = ff[0]\n","\n","        in_band_flux = []\n","        for i in range(len(wave)):\n","          if wave[i] >= min_wave and wave[i] <= max_wave:\n","            in_band_flux.append(f_nu[i])\n","\n","        if in_band_flux:\n","\n","          mean_flux = np.mean(in_band_flux)\n","          if mean_flux < 0 : mean_flux = -np.inf\n","          mg_sp = -2.5*np.log10(mean_flux) - 48.6\n","\n","          cor_mag = [self.corr_mag[i] for i in range(len(self.corr_mag)) if filtro == self.filterlist[i]]\n","          if math.isnan(cor_mag[0]) == False:\n","            mg_sp = mg_sp - cor_mag[0]\n","          if math.isfinite(mg_sp):\n","            extraSP   = np.append(extraSP, mg_sp)\n","            extraLC   = np.append(extraLC, mg_lc)\n","            extraER   = np.append(extraER, err_lc)\n","\n","          if cut == True:\n","            save_sp   = np.array([])\n","            save_lc   = np.array([])\n","            save_err  = np.array([])\n","            for i in range(len(extraER)):\n","              if extraER[i] < 10*np.mean(ERR):\n","                if abs(extraSP[i]-extraLC[i]) < 2:\n","                  save_sp   = np.append(save_sp, extraSP[i])\n","                  save_lc   = np.append(save_lc, extraLC[i])\n","                  save_err  = np.append(save_err, extraER[i])\n","            extraSP  = save_sp\n","            extraLC  = save_lc\n","            extraER = save_err\n","\n","    return extraSP, extraLC, extraER\n","\n","\n","  def calibration(self, SP, LC, ER, extraSP, extraLC, extraER):\n","\n","    tot_SP  = np.append(SP, extraSP)\n","    tot_LC  = np.append(LC, extraLC)\n","    tot_ER  = np.append(ER, extraER)\n","    m0, q0  = linear_fit(tot_SP, tot_LC, tot_ER)\n","    uncal_SP= (tot_LC - q0) / m0\n","    m1, q1  = linear_fit(uncal_SP, tot_LC, tot_ER)\n","    cal_SP  = (uncal_SP * m1 ) + q1\n","\n","    return cal_SP, uncal_SP, tot_LC, tot_ER\n","\n","\n","  def counting_filters(self, lc_counter, sp_counter, total_counter):\n","\n","    ordered_filters = [\n","        'UVW2', 'UVM2', 'UVW1', \"u'\", 'u', 'U', \"U'\", 'C', 'v', 'B', \"B'\", 'b', 'g', \"g'\", 'V', 'y',\n","        \"v'\", 'G', \"V'\", 'r', \"r'\", 'R', 'Rc', 'i', \"i'\", 'I', 'z', 'Ic', \"z'\", 'Z', 'Y', 'J', 'H',\n","        'K', 'Ks', 'I1', 'I2'\n","    ]\n","\n","    lcc       = Counter(list(chain(*lc_counter)))\n","    spc       = Counter(list(chain(*sp_counter)))\n","    ttc       = Counter(list(chain(*total_counter)))\n","    sorted_lcc = {key: lcc[key] for key in ordered_filters if key in lcc}\n","    sorted_spc = {key: spc[key] for key in ordered_filters if key in spc}\n","    sorted_ttc = {key: ttc[key] for key in ordered_filters if key in ttc}\n","\n","\n","    return sorted_lcc, sorted_spc, sorted_ttc\n","\n","\n","  def define_the_bandwidth(self, lc_counter, sp_counter, total_counter):\n","\n","    _, _, sorted_ttc  =  self.counting_filters(lc_counter, sp_counter, total_counter)\n","\n","    wavs = []\n","\n","    for key, value in sorted_ttc.items():\n","        if value > 10 and key in self.filterlist:\n","            idx = filterlist.index(key)\n","            wavs.append(self.centroid[idx] - self.fwhm[idx] / 2)\n","\n","    min_w = min(wavs)\n","    max_w = max(wavs)\n","\n","    return min_w, max_w\n","\n","\n","  def plot_histogram(self, lc_counter, sp_counter, total_counter):\n","\n","    sorted_lcc, sorted_spc, sorted_ttc = self.counting_filters(lc_counter, sp_counter, total_counter)\n","\n","    df1 = pd.DataFrame(list(sorted_lcc.items()), columns=['Value', 'Count'])\n","    df2 = pd.DataFrame(list(sorted_spc.items()), columns=['Value', 'Count'])\n","    df3 = pd.DataFrame(list(sorted_ttc.items()), columns=['Value', 'Count'])\n","    df1 = df1.set_index('Value')\n","    df2 = df2.set_index('Value')\n","    df3 = df3.set_index('Value')\n","\n","    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(16, 11), sharex=True)\n","\n","\n","    dataframes = [df1, df2, df3]\n","\n","    for i, ax in enumerate(axes):\n","        ax.grid()\n","        sub_df = dataframes[i]\n","        band_colors = [get_color_band(band_name) for band_name in sub_df.index]\n","        ax.bar(sub_df.index, sub_df['Count'], color=band_colors)\n","        ax.set_xlabel('')\n","        ax.set_ylabel('')\n","        ax.tick_params(axis='x', rotation=45, labelsize=18)\n","        ax.tick_params(axis='y', labelsize=18)\n","        for tick in ax.get_yticklabels():\n","          tick.set_fontweight('bold')\n","\n","        if i < 2:\n","            ax.set_xticks([])\n","\n","\n","    axes[2].set_xticks(df3.index)\n","    for tick in axes[2].get_xticklabels():\n","        tick.set_fontweight('bold')\n","\n","    axes[0].text(0.99, 0.97, 'Light Curves', transform=axes[0].transAxes, ha='right', va='top', fontsize=25, fontweight='bold')\n","    axes[1].text(0.99, 0.97, 'Spectra', transform=axes[1].transAxes, ha='right', va='top', fontsize=25, fontweight='bold')\n","    axes[2].text(0.99, 0.97, 'Matched', transform=axes[2].transAxes, ha='right', va='top', fontsize=25, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":597,"status":"ok","timestamp":1703672592906,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"a_sgGTZa86ru"},"outputs":[],"source":["'''\n","Here I apply the study on filters.\n","\n","if execute == True I run the code on every supernova\n","if execute == False I define immediatly the confident value\n","\n","Until the case-of-study catalogue is not modified,\n","mantain execute = False: the results are already given\n","\n","'''\n","\n","execute = False\n","lc_counter, sp_counter, total_counter      =  [],   [],   []\n","aux1, aux2, aux3                           = ([]), ([]), ([])\n","\n","if execute:\n","\n","  for name in SN_names:\n","    de_class = data_extraction(name, data_path, acc, tresh, bandpass)\n","    files, sp_set, lc_filters, lc_set, sp_filters, matched  = de_class.total_exploration()\n","\n","    lc_counter.append(lc_filters)\n","    sp_counter.append(sum(sp_filters.values(), [] ))\n","    total_counter.append(matched)\n","\n","  cal           = calibration(name, files, sp_set, lc_filters, lc_set, matched, bandpass)\n","\n","  min_w, max_w  = cal.define_the_bandwidth(lc_counter, sp_counter, total_counter)\n","\n","  cal.plot_histogram(lc_counter, sp_counter, total_counter)\n","\n","else:\n","\n","  min_w = 3000\n","  max_w = 9000\n","\n","\n","print('The interval of wavelength is [', min_w, '-', max_w, ']AA')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1703672592906,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"iWK5F1VZi1tE","outputId":"bc239fa8-2115-417e-cda1-33e4e987aa0c"},"outputs":[],"source":["'''\n","Here I calibrate light curves and spectra to obtain a\n","set of calibration parameters: mean_m and mean_q\n","\n","if execute == True I run the code on every supernova\n","if execute == False I define the parameters by past results\n","\n","Until the case-of-study catalogue is not modified,\n","mantain execute = False: the results are already given\n","'''\n","\n","execute = False\n","cut     = False\n","if execute:\n","\n","  mean_m, mean_q = ([]) , ([])\n","  SP, LC, ER     = ([]), ([]), ([])\n","\n","  for name in SN_names:\n","      de_class    = data_extraction(name, data_path, acc, tresh, bandpass)\n","      files, sp_set, lc_filters, lc_set, _ , matched  = de_class.total_exploration()\n","      cal         = calibration(name, files, sp_set, lc_filters, lc_set, matched, bandpass)\n","      gp_set      = cal.gaussian_process()\n","      x, y, yerr  = cal.mangling(gp_set, cut = cut)\n","      m0, q0      = linear_fit(x, y, yerr)\n","\n","      SP = np.append(SP, x)\n","      LC = np.append(LC, y)\n","      ER = np.append(ER, yerr)\n","\n","      mean_m, mean_q = linear_fit(SP, LC, ER)\n","      plt.plot(SP, LC, '.', color='cyan')\n","      plt.plot(SP, linear_fun(SP, mean_m, mean_q), '-', color='red')\n","\n","else:\n","  mean_m, mean_q = 0.892407, 0.891162\n","\n","print('The calibration parameters are m = %f'%mean_m, 'and q = %f'%mean_q)"]},{"cell_type":"markdown","metadata":{"id":"w23DZ4yta6c_"},"source":["# Section 2: chi-squared test comparison"]},{"cell_type":"markdown","metadata":{"id":"urt1ns78ztM8"},"source":["I'm testing the capabilities of CASTOR on a new supernova, outside the catalogue of study. Right now, I'm focusing on SN2015ap.\n","\n","\n","If you want to study a different supernova, you need to add your data in the directory specified by \"new_path\". This file has to be in .dat format, with three columns: epoch, magnitude, error and filter."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1703675792040,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"g4aDIdkYK_TS"},"outputs":[],"source":["new_name              = 'SN2015ap'\n","new_path              = LOCALPATH + 'Application' + \"/\" + new_name + \"/\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":251,"status":"ok","timestamp":1703675794046,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"qOF89gAmOeAM"},"outputs":[],"source":["def read_lightcurves(name, path):\n","\n","  name_file = path + name + \".dat\"\n","  lc_set   = defaultdict(int)\n","  time_array  = ([])\n","  mag_array   = ([])\n","  emag_array  = ([])\n","  band_list   = []\n","\n","  with open(name_file, 'r') as file:\n","      for line in file:\n","          parts = line.strip().split()\n","          time = float(parts[0])\n","          mag  = float(parts[1])\n","          emag = float(parts[2])\n","          band = parts[3]\n","\n","          time_array = np.append(time_array, time)\n","          mag_array  = np.append(mag_array, mag)\n","          emag_array = np.append(emag_array, emag)\n","          band_list.append(band)\n","\n","  lc_filters = [band_list[0]]\n","  for i in range(len(band_list)-1):\n","    if band_list[i] != band_list[i+1]:\n","      lc_filters.append(band_list[i+1])\n","\n","  for filtro in lc_filters:\n","    lc_set['time_%s'%filtro]  = [time_array[index]   for index, val in enumerate(band_list) if val == filtro]\n","    lc_set['mag_%s' %filtro]  = [mag_array[index]    for index, val in enumerate(band_list) if val == filtro]\n","    lc_set['emag_%s'%filtro]  = [emag_array[index]   for index, val in enumerate(band_list) if val == filtro]\n","\n","  return lc_filters, lc_set\n","\n","def toe(lc_set, lc_filters):\n","\n","  t0, t1, dt = ([]), ([]), ([])\n","\n","  for filtro in lc_filters:\n","    time = np.array(lc_set['time_' + filtro])\n","    t1   = np.append(t1, time[0])\n","    dt   = np.append(dt, avg_distance(time[0:10]))\n","    t0   = np.append(t0, t1-dt)\n","\n","  min_value = np.min(t0)\n","  std_value = np.std(t0)\n","\n","  return min_value\n","\n","def rescale_time(lc_filters, lc_set, t0):\n","\n","  new_set = {}\n","\n","  for filtro in lc_filters:\n","    time                      = np.array(lc_set['time_' + filtro])\n","    mag                       = np.array(lc_set['mag_' + filtro])\n","    emag                      = np.array(lc_set['emag_' + filtro])\n","    scale_time                = time - t0\n","    scale_time, mag, emag     = order_sorting(scale_time, mag, emag)\n","    new_set['time_%s'%filtro] = scale_time\n","    new_set['mag_%s'%filtro]  = mag\n","    new_set['emag_%s'%filtro] = emag\n","\n","  return new_set\n","\n","\n","def gaussian_process(common_filters, lc_set_obs, lc_set_new):\n","\n","  gp_set = defaultdict(int)\n","  for filtro in common_filters:\n","\n","    ref_time    = np.array(lc_set_new['time_' + filtro])\n","\n","    x           = np.array(lc_set_obs['time_' + filtro])\n","    y           = np.array(lc_set_obs['mag_'  + filtro])\n","    yerr        = np.array(lc_set_obs['emag_' + filtro])\n","    amplitude   = np.mean(y)\n","\n","    lengthscale0 = np.mean(x)\n","    lengthscale1 = np.min(sampling_step(x))\n","    lengthscale2 = np.max(sampling_step(x))\n","\n","    k0           = amplitude * Matern32Kernel(lengthscale0)\n","    k1           = amplitude * Matern32Kernel(lengthscale1)\n","    k2           = amplitude * Matern32Kernel(lengthscale2)\n","\n","    kernel       = k1 + k2 + k0\n","\n","    gp           = george.GP(kernel)\n","    star         = gp.compute(x, yerr)\n","\n","    p0           = gp.get_parameter_vector()\n","    results      = op.minimize(nll, p0, args = (y, gp, star), jac=grad_nll, method=\"L-BFGS-B\")\n","    gp.set_parameter_vector(results.x)\n","\n","    mu, cov      = gp.predict(y, ref_time)\n","    std          = np.sqrt(cov.diagonal())\n","\n","    gp_set['time_%s'%filtro]    = ref_time\n","    gp_set['mag_%s'%filtro]     = mu\n","    gp_set['std_%s'%filtro]     = std\n","\n","  return gp_set\n","\n","\n","def comparison(name_obs, name_new, path_obs, path_new):\n","\n","  obs_filters, obs_set = read_lightcurves(name_obs, path_obs)\n","  new_filters, new_set = read_lightcurves(name_new, path_new)\n","\n","  common_filters = set(obs_filters) & set(new_filters)\n","\n","  t0_obs = toe(obs_set, obs_filters)\n","  t0_new = toe(new_set, new_filters)\n","\n","  obs_set = rescale_time(obs_filters, obs_set, t0_obs)\n","  new_set = rescale_time(new_filters, new_set, t0_new)\n","\n","  gp_set  = gaussian_process(common_filters, obs_set, new_set)\n","\n","  chi2_total_norm = ([])\n","  len_filters     = ([])\n","  chi2_total      = ([])\n","\n","  for filtro in common_filters:\n","      mag1  = gp_set['mag_' + filtro]\n","      mag2  = np.array(new_set['mag_' + filtro])\n","      mag2  = np.sum(mag1)/np.sum(mag2) * mag2\n","\n","      chi2_single, _   = chisquare(f_obs=mag1, f_exp=mag2)\n","      chi2_total_norm  = np.append(chi2_total_norm, chi2_single/len(mag2))\n","      chi2_total       = np.append(chi2_total, chi2_single)\n","\n","\n","  total_chi2          = np.sum(chi2_total_norm)\n","  total_chi2_norm     = total_chi2 / len(common_filters)\n","  len_filters         = np.append(len_filters, len(common_filters))\n","\n","  print(name_obs,'\\t', 'chi2 total: ', round(total_chi2, 4), '\\t', 'normalized chi2:', round(total_chi2_norm,4),'\\t',  'len filters: ', len_filters)\n","\n","  return (total_chi2_norm, len_filters)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177768,"status":"ok","timestamp":1703675981028,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"te61oflBO7dc","outputId":"4a72d0ab-a321-4e6d-a8ff-3958b1a21922"},"outputs":[],"source":["'''\n","Here I compare the new supernova\n","with every supernova in the case-of-study (COS) catalogue\n","\n","Until the case-of-study catalogue doesn't change (or I don't add new data)\n","the best match for SN2015ap is iPTF13bvn\n","'''\n","\n","execute = False\n","\n","if execute:\n","\n","  new_filters, new_set  = read_lightcurves(new_name, new_path)\n","  COS_path              = data_path + 'data_lightcurves' + \"/\"\n","  all_chi2              = ([])\n","  saved_chi2            = ([])\n","  saved_filt            = ([])\n","  saved_name            = []\n","\n","\n","  for COS_name in SN_names:\n","\n","    chi2, len_filters  = comparison(COS_name, new_name, COS_path, new_path)\n","    all_chi2           = np.append(all_chi2, chi2)\n","    saved_filt         = np.append(saved_filt, len_filters)\n","\n","\n","  index = ([])\n","  for i in range(len(saved_filt)):\n","    if saved_filt[i] >= 0.7 * len(new_filters):\n","\n","      saved_chi2 = np.append(saved_chi2, all_chi2[i])\n","      saved_name.append(SN_names[i])\n","\n","  best_guess = np.argmin(saved_chi2)\n","  final_name = saved_name[best_guess]\n","  final_chi  = saved_chi2[best_guess]\n","\n","else:\n","  if new_name == 'SN2015ap':\n","    final_name  = 'SN2011hs'\n","    final_chi   = 0.003986352577366923\n","  if new_name == 'SN2023ixf':\n","    final_name  = 'SN2013ej'\n","    final_chi   = 0\n","\n","print('The best match for %s'%new_name, 'is:%s'%final_name, ' with a total chi-squared of', round(final_chi,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":13117,"status":"ok","timestamp":1703675994105,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"T5EUDYM0iJYL","outputId":"79751353-248a-411d-ead6-e851a56341da"},"outputs":[],"source":["'''\n","Here I directly compare the new supernova\n","with the best match, plotting and saving the comparison graphs\n","'''\n","\n","name1 = final_name\n","path1 = data_path + 'data_lightcurves' + \"/\"\n","\n","name2 = new_name\n","path2 = new_path\n","\n","lc_filters_1, lc_set_1 = read_lightcurves(name1, path1)\n","lc_filters_2, lc_set_2 = read_lightcurves(name2, path2)\n","\n","common_filters = set(lc_filters_1) & set(lc_filters_2)\n","\n","t0_1 = toe(lc_set_1, lc_filters_1)\n","t0_2 = toe(lc_set_2, lc_filters_2)\n","\n","lc_set_1 = rescale_time(lc_filters_1, lc_set_1, t0_1)\n","lc_set_2 = rescale_time(lc_filters_2, lc_set_2, t0_2)\n","\n","gp_set_1 = gaussian_process(common_filters, lc_set_1, lc_set_2)\n","\n","num_filters = len(common_filters)\n","num_rows    = (num_filters + 1) // 2  \n","fig, axs    = plt.subplots(num_rows, 2, figsize=(8.27, 11.69)) \n","fig.tight_layout(pad=3.0)\n","\n","deltam = np.array([])\n","chi = np.array([])\n","\n","for i, filtro in enumerate(common_filters):\n","    mag1 = gp_set_1['mag_' + filtro]\n","    emag1 = gp_set_1['emag_' + filtro]\n","    time1 = gp_set_1['time_' + filtro]\n","    mag2 = np.array(lc_set_2['mag_' + filtro])\n","    emag2 = np.array(lc_set_2['emag_' + filtro])\n","    time2 = lc_set_2['time_' + filtro]\n","    dm = np.sum(mag1) / np.sum(mag2)\n","    mag2 = dm * mag2\n","    chi2_statistic, _ = chisquare(f_obs=mag1, f_exp=mag2)\n","\n","    deltam = np.append(deltam, dm)\n","    chi = np.append(chi, chi2_statistic / len(time2))\n","\n","    row = i // 2  # Determine the row for the subplot\n","    col = i % 2   # Determine the column for the subplot\n","\n","    ax = axs[row, col]\n","    ax.grid()\n","\n","    if filtro == 'B':\n","        ax.plot(time1, mag1, color='blue', label=name1)\n","        ax.errorbar(time2, mag2, emag2, fmt='o', color='red', label=name2)\n","    else:\n","        ax.plot(time1, mag1, color='blue')\n","        ax.errorbar(time2, mag2, emag2, fmt='o', color='red')\n","\n","    ax.set_xlabel('Days after explosion', fontsize=12, fontweight='bold')\n","    ax.set_ylabel('Magnitude', fontsize=12, fontweight='bold')\n","    ax.invert_yaxis()\n","\n","    ax.text(0.78, 0.9, f'Filter {filtro}', transform=ax.transAxes,\n","            fontsize=12, fontweight='bold', ha='center', va='center')\n","    ax.text(0.8, 0.8, f'$\\chi^2$ = {round(chi[i], 3)}', transform=ax.transAxes,\n","            fontsize=12, fontweight='bold', ha='center', va='center')\n","    ax.text(0.8, 0.7, f'$\\Delta$m = {round(deltam[i], 3)}', transform=ax.transAxes,\n","            fontsize=12, fontweight='bold', ha='center', va='center')\n","\n","fig.legend(loc='upper right', fontsize=12)\n","\n","fig_name = new_path + 'Comparison.png'\n","plt.savefig(fig_name)\n","plt.show()\n","plt.close()\n"]},{"cell_type":"markdown","metadata":{"id":"yg7YlfoHcd78"},"source":["# Section 3: building templates"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":288,"status":"ok","timestamp":1703676181038,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"94er4vl2W2jp"},"outputs":[],"source":["class build_templates():\n","\n","  def __init__(self, name, path, bandpass, min_w, max_w, mean_m, mean_q, ref_files, ref_sp_set, ref_t0):\n","\n","    self.name       = name\n","    self.path       = path\n","    self.filterlist = bandpass['filterlist']\n","    self.centroid   = bandpass['centroid']\n","    self.fwhm       = bandpass['fwhm']\n","    self.min_w      = min_w\n","    self.max_w      = max_w\n","    self.m          = mean_m\n","    self.q          = mean_q\n","    self.ref_files  = ref_files\n","    self.ref_sp_set = ref_sp_set\n","    self.ref_t0     = ref_t0\n","\n","  def read_lightcurves(self):  \n","\n","    name_file = self.path + self.name + \".dat\"\n","    lc_set    = defaultdict(int)\n","    time_array  = ([])\n","    mag_array   = ([])\n","    emag_array  = ([])\n","    band_list   = []\n","\n","    with open(name_file, 'r') as file:\n","        for line in file:\n","            parts = line.strip().split()\n","            time = float(parts[0])\n","            mag  = float(parts[1])\n","            emag = float(parts[2])\n","            band = parts[3]\n","\n","            time_array = np.append(time_array, time)\n","            mag_array  = np.append(mag_array, mag)\n","            emag_array = np.append(emag_array, emag)\n","            band_list.append(band)\n","\n","    lc_filters = [band_list[0]]\n","    for i in range(len(band_list)-1):\n","      if band_list[i] != band_list[i+1]:\n","        lc_filters.append(band_list[i+1])\n","\n","    for filtro in lc_filters:\n","      lc_set['time_%s'%filtro]  = [time_array[index]   for index, val in enumerate(band_list) if val == filtro]\n","      lc_set['mag_%s' %filtro]  = [mag_array[index]    for index, val in enumerate(band_list) if val == filtro]\n","      lc_set['emag_%s'%filtro]  = [emag_array[index]   for index, val in enumerate(band_list) if val == filtro]\n","\n","\n","    return (lc_filters, lc_set)\n","\n","\n","  def t0(self, lc_filters, lc_set): \n","\n","    t0, t1, dt = ([]), ([]), ([])\n","\n","    for filtro in lc_filters:\n","      time = np.array(lc_set['time_' + filtro])\n","      t1   = np.append(t1, time[0])\n","      dt   = np.append(dt, avg_distance(time[0:10]))\n","      t0   = np.append(t0, t1-dt)\n","\n","    min_value = np.min(t0)\n","\n","    return min_value\n","\n","\n","  def scale_lcs(self, lc_filters, lc_set, t0): \n","\n","    new_set            = {}\n","    for filtro in lc_filters:\n","      time                      = np.array(lc_set['time_' + filtro])\n","      mag                       = np.array(lc_set['mag_' + filtro])\n","      emag                      = np.array(lc_set['emag_' + filtro])\n","      scale_time                = time - t0\n","      scale_time, mag, emag     = order_sorting(scale_time, mag, emag)\n","      new_set['time_%s'%filtro] = scale_time\n","      new_set['mag_%s'%filtro]  = mag\n","      new_set['emag_%s'%filtro] = emag\n","\n","    return lc_filters, new_set\n","\n","\n","  def gaussian_process_on_lcs(self, lc_filters, new_set): \n","\n","    gp_set = defaultdict(int)\n","\n","    for filtro in lc_filters:\n","      x           = np.array(new_set['time_' + filtro])\n","      y           = np.array(new_set['mag_'  + filtro])\n","      yerr        = np.array(new_set['emag_' + filtro])\n","\n","\n","      x           = np.append(x, 200)\n","      y           = np.append(y, 25)\n","      yerr        = np.append(yerr, 0.1*25)\n","\n","      amplitude   = np.mean(y)\n","\n","      lengthscale0 = np.mean(x)\n","      lengthscale1 = np.min(sampling_step(x))\n","      lengthscale2 = np.max(sampling_step(x))\n","\n","      k0           = amplitude * Matern32Kernel(lengthscale0)\n","      k1           = amplitude * Matern32Kernel(lengthscale1)\n","      k2           = amplitude * Matern32Kernel(lengthscale2)\n","\n","      kernel       = k1 + k2 + k0\n","\n","      gp          = george.GP(kernel)\n","      star        = gp.compute(x, yerr)\n","\n","      p0          = gp.get_parameter_vector()\n","      results     = op.minimize(nll, p0, args = (y, gp, star), jac=grad_nll, method=\"L-BFGS-B\")\n","      gp.set_parameter_vector(results.x)\n","\n","      t           = np.linspace(0, 200, len(y))\n","      mu, cov     = gp.predict(y, t)\n","      std         = np.sqrt(cov.diagonal())\n","\n","      eff_wav = [self.centroid[i] for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","      wav_c   = np.repeat(eff_wav[0], len(mu))\n","\n","      new_mu  = (mu - self.q) / self.m\n","      flux    = light_vel_A * 10**(- 0.4* (new_mu + 48.6)) / (eff_wav[0] **2)\n","\n","      gp_set['flux_%s'%filtro]  = flux\n","      gp_set['wave_%s'%filtro]  = wav_c\n","      gp_set['time_%s'%filtro]  = t\n","      gp_set['mag_%s'%filtro]   = mu\n","      gp_set['std_%s'%filtro]   = std\n","\n","    return gp_set\n","\n","\n","  def injection(self, lc_filters, gp_set):  \n","\n","    total_flux  = ([])\n","    total_wave  = ([])\n","    total_time  = ([])\n","\n","    for filtro in lc_filters:\n","      total_flux = np.append(total_flux, gp_set['flux_'+filtro])\n","      total_wave = np.append(total_wave, gp_set['wave_'+filtro])\n","      total_time = np.append(total_time, gp_set['time_'+filtro])\n","\n","    return total_flux, total_wave, total_time\n","\n","\n","  def scale_real_spectra(self):     \n","\n","    scaled_files = self.ref_files - self.ref_t0\n","    scaled_set   = defaultdict(int)\n","\n","    for epoch in self.ref_files:\n","      flux = self.ref_sp_set['flux_' + str(epoch)]\n","      wave = self.ref_sp_set['wave_' + str(epoch)]\n","\n","      scaled_set['flux_%s'%str(epoch-self.ref_t0)] = flux\n","      scaled_set['wave_%s'%str(epoch-self.ref_t0)] = wave\n","\n","    return scaled_files, scaled_set\n","\n","  def under_files(self, scaled_files):\n","\n","    under_files = ([])\n","    for epoch in scaled_files:\n","      if epoch <= 200:\n","        under_files = np.append(under_files, epoch)\n","\n","    return under_files\n","\n","\n","  def cut_real_spectra(self, total_flux, scaled_set, under_files):\n","\n","    cut_epoch = 1\n","    go        = True\n","\n","    limit_points = 5000\n","\n","    while go:\n","        cut_files = under_files[0::cut_epoch]\n","        all_points = []\n","\n","        for epoch in cut_files:\n","            obs_flux = scaled_set['flux_' + str(epoch)]\n","            all_points = np.append(all_points, obs_flux)\n","\n","        if len(all_points)+len(total_flux) > limit_points:\n","            medium_points = len(all_points) / len(cut_files)\n","            medium_desire = limit_points / len(cut_files)\n","            cut_points = math.ceil(medium_points / medium_desire)\n","            total_points = all_points[0::cut_points]\n","\n","            if len(total_points)+len(total_flux) > limit_points:\n","                cut_epoch += 1\n","            else:\n","                go = False\n","        else:\n","            go = False\n","\n","    return cut_epoch, cut_points\n","\n","\n","  def total_spectra(self, total_flux, total_time, total_wave, under_files, scaled_set, cut_epoch, cut_points):\n","\n","    add_obs_epoch = ([])\n","\n","    for epoch in under_files[0::cut_epoch]:\n","      obs_wave      = scaled_set['wave_'+str(epoch)]\n","      obs_flux      = scaled_set['flux_'+str(epoch)]\n","      obs_time      = np.repeat(epoch, len(obs_flux))\n","      total_flux    = np.append(total_flux, obs_flux[0::cut_points])\n","      total_wave    = np.append(total_wave, obs_wave[0::cut_points])\n","      total_time    = np.append(total_time, obs_time[0::cut_points])\n","      add_obs_epoch = np.append(add_obs_epoch, epoch)\n","\n","    return total_flux, total_wave, total_time, add_obs_epoch\n","\n","  def get_total_time(self, add_obs_epoch):\n","\n","    min_time                = int(0)\n","    max_time                = int(200)\n","\n","    index       = 100 - len(add_obs_epoch)\n","    step        = (max_time - min_time) / (index-1)\n","    time_series = np.arange(min_time, max_time + step, step=step, dtype=int)\n","\n","    new_time_series = sorted(time_series.tolist() + add_obs_epoch.tolist())\n","\n","    return new_time_series\n","\n","\n","  def bi_gaussian_process(self, total_flux, total_wave, total_time):\n","\n","    x    = total_time\n","    y    = total_flux\n","    z    = total_wave\n","    yerr = y * 0.1\n","\n","    y_scale     = np.mean(y)\n","    z_scale     = 3000\n","    x_scale_1   = np.min(sampling_step(x))\n","    x_scale_2   = np.max(sampling_step(x))\n","\n","    k1          = y_scale * Matern32Kernel([x_scale_1,z_scale], ndim = 2)\n","    k2          = y_scale * Matern32Kernel([x_scale_2,z_scale], ndim = 2)\n","\n","    kernel      = k1 + k2\n","\n","    gp          = george.GP(kernel)\n","    p0          = gp.get_parameter_vector()\n","\n","    d2d         = np.matrix([x, z])\n","    star        = gp.compute(d2d.T, yerr)\n","\n","    results     = op.minimize(nll, p0, args = (y, gp, star), jac=grad_nll, method=\"L-BFGS-B\")\n","    gp.set_parameter_vector(results.x)\n","\n","    return gp\n","\n","\n","\n","  def prediction(self, gp, new_time_series, total_flux, total_wave, total_time):\n","\n","    pred_set    = defaultdict(int)\n","\n","    x                                     = np.array(total_time)\n","    y                                     = np.array(total_flux)\n","    z                                     = np.array(total_wave)\n","\n","    for epoch in new_time_series:\n","      x_pred     = epoch\n","      z_pred     = np.linspace(min_w, max_w, len(y))\n","      tot_pred   = np.vstack([np.full_like(z_pred, x_pred), z_pred]).T\n","      pred_mean, pred_var = gp.predict(y, tot_pred, return_var=True)\n","\n","      pred_set['flux_%s'%str(epoch)]  = pred_mean\n","      pred_set['eflux_%s' %str(epoch)]  = np.sqrt(pred_var)\n","      pred_set['wave_%s' %str(epoch)]  = z_pred\n","\n","    return (pred_set, new_time_series)\n","\n","\n","  def scale_factor(self):\n","\n","    ref_ofm_15    = ([])\n","    ref_ofm_30    = ([])\n","    ref_ofm_60    = ([])\n","    ref_ofm_100   = ([])\n","    ref_ofm_130   = ([])\n","    ref_ofm_160   = ([])\n","    ref_ofm_200   = ([])\n","\n","\n","    for epoch in self.ref_files:\n","\n","      ref_time   = epoch - self.ref_t0\n","      ref_flux   = np.array(self.ref_sp_set['flux_'+str(epoch)])\n","      mean_order = magnitude_order(np.mean(ref_flux))\n","\n","      if ref_time <=15:  ref_ofm_15   = np.append(ref_ofm_15, mean_order)\n","      if ref_time <=30:  ref_ofm_30   = np.append(ref_ofm_30, mean_order)\n","      if ref_time <=60:  ref_ofm_60   = np.append(ref_ofm_60, mean_order)\n","      if ref_time <=100: ref_ofm_100  = np.append(ref_ofm_100, mean_order)\n","      if ref_time <=130: ref_ofm_130  = np.append(ref_ofm_130, mean_order)\n","      if ref_time <=160: ref_ofm_160  = np.append(ref_ofm_160, mean_order)\n","      if ref_time >160:  ref_ofm_200  = np.append(ref_ofm_200, mean_order)\n","\n","    ref_ofm_15  = new_mean(ref_ofm_15)\n","    ref_ofm_30  = new_mean(ref_ofm_30)\n","    ref_ofm_60  = new_mean(ref_ofm_60)\n","    ref_ofm_100 = new_mean(ref_ofm_100)\n","    ref_ofm_130 = new_mean(ref_ofm_130)\n","    ref_ofm_160 = new_mean(ref_ofm_160)\n","    ref_ofm_200 = new_mean(ref_ofm_200)\n","\n","\n","    offset = np.array([ref_ofm_15, ref_ofm_30, ref_ofm_60, ref_ofm_100, ref_ofm_130, ref_ofm_160, ref_ofm_200])\n","    media  = int(np.mean(offset[offset!=0]))\n","\n","    for i in range(len(offset)):\n","      if offset[i] == 0:\n","        offset[i] = media\n","\n","    return offset\n","\n","  def add_offset(self, offset, pred_set, new_time_series, t0):\n","\n","    final_time_series = new_time_series + t0\n","    final_set         = defaultdict(int)\n","\n","    for epoch in new_time_series:\n","      syn_flux = np.array(pred_set['flux_'  + str(epoch)])\n","      syn_err  = np.array(pred_set['eflux_' + str(epoch)])\n","      syn_ofm  = abs(magnitude_order(np.mean(syn_flux)))\n","\n","      new_flux = syn_flux * 10**syn_ofm\n","      new_err  = syn_err  * 10**syn_ofm\n","\n","      if epoch <=15:   new_mean = new_flux * 10**offset[0]; new_std = new_err * 10**offset[0]\n","      if epoch <=30:   new_mean = new_flux * 10**offset[1]; new_std = new_err * 10**offset[1]\n","      if epoch <=60:   new_mean = new_flux * 10**offset[2]; new_std = new_err * 10**offset[2]\n","      if epoch <=100:  new_mean = new_flux * 10**offset[3]; new_std = new_err * 10**offset[3]\n","      if epoch <=130:  new_mean = new_flux * 10**offset[4]; new_std = new_err * 10**offset[4]\n","      if epoch <=160:  new_mean = new_flux * 10**offset[5]; new_std = new_err * 10**offset[5]\n","      if epoch >160:   new_mean = new_flux * 10**offset[6]; new_std = new_err * 10**offset[6]\n","\n","      new_epoch = epoch + t0\n","\n","      pred_wav = pred_set['wave_' + str(epoch)]\n","\n","      final_set['flux_%s'  %str(new_epoch)]   = new_mean\n","      final_set['eflux_%s' %str(new_epoch)]   = new_std\n","      final_set['wave_%s'  %str(new_epoch)]   = pred_wav\n","\n","    return final_set, final_time_series\n","\n","\n","  def build_and_save(self, new_name, new_path, save = False):\n","\n","    lc_filters, lc_set                 = self.read_lightcurves()\n","    t0                                 = self.t0(lc_filters, lc_set)\n","    lc_filters, new_set                = self.scale_lcs(lc_filters, lc_set, t0)\n","    gp_set                             = self.gaussian_process_on_lcs(lc_filters, new_set)\n","    total_flux, total_wave, total_time = self.injection(lc_filters, gp_set)\n","    scaled_files, scaled_set           = self.scale_real_spectra()\n","    under_files                        = self.under_files(scaled_files)\n","    cut_epoch, cut_points              = self.cut_real_spectra(total_flux, scaled_set, under_files)\n","    total_flux, total_wave, total_time, add_obs_epoch = self.total_spectra(total_flux, total_time, total_wave, under_files,\n","                                                                           scaled_set, cut_epoch, cut_points)\n","    new_time_series                    = self.get_total_time(add_obs_epoch)\n","    gp                                 = self.bi_gaussian_process(total_flux, total_wave, total_time) \n","    print('First check: gaussian process')\n","    pred_set, new_time_series          = self.prediction(gp, new_time_series, total_flux, total_wave, total_time)\n","    print('Second check: prediction') \n","    offset                             = self.scale_factor()\n","    final_set, final_time_series       = self.add_offset(offset, pred_set, new_time_series, t0)\n","\n","    if save == True:\n","\n","      new_folder = new_path + 'Templates' + \"/\"\n","\n","      if not os.path.exists(new_folder):\n","        os.makedirs(new_folder)\n","\n","      #Create the data files\n","\n","      for epoch in final_time_series:\n","        flux = final_set['flux_'+str(epoch)]\n","        wave = final_set['wave_'+str(epoch)]\n","        std  = final_set['eflux_'+str(epoch)]\n","        filename  = new_name + \"_\" + str(epoch)\n","        matrice   = np.column_stack((wave, flux, std))\n","        file_path = os.path.join(new_folder, f\"{filename}.dat\")\n","        np.savetxt(file_path, matrice, delimiter='\\t', fmt='%s')\n","\n","      #Create the plots\n","\n","      i = 0\n","      plt.figure(figsize=(8.27, 11.69))\n","      plt.grid()\n","      cut_series = final_time_series[0::5]\n","      for epoch in cut_series:\n","        flux = final_set['flux_' + str(epoch)]\n","        wave = final_set['wave_' + str(epoch)]\n","        plt.plot(wave, np.log(flux.astype('float64')) - 10 * i, color='blue')\n","        plt.xlabel('Wavelength [$\\AA$]', fontsize=12, fontweight='bold')\n","        plt.ylabel('log(flux) - offset', fontsize=12, fontweight='bold')\n","        if epoch == cut_series[0] or epoch == cut_series[len(cut_series) // 2] or epoch == cut_series[-1]:\n","          plt.text(wave[-1] + 50, np.log(flux[-1]) - 10 * i, f'+ {int(epoch-t0)} days', fontsize=10, fontweight='bold', va='center')\n","        i += 1\n","\n","      fig_name = new_path + 'templates.png'\n","      plt.savefig(fig_name)\n","      plt.close()\n","\n","\n","    return (final_set, final_time_series)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":29696,"status":"ok","timestamp":1703676211022,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"4hoRPNQjrKoR"},"outputs":[],"source":["'''\n","Here I define the parameters of the reference supernova.\n","'''\n","\n","ref_name                                       = final_name\n","ref_path                                       = data_path + 'data_lightcurves' + \"/\"\n","ref_files, ref_sp, ref_filters, ref_lcs, _ , _ =  data_extraction(ref_name, data_path, acc, tresh, bandpass).total_exploration()\n","ref_t0                                         = toe(ref_lcs, ref_filters)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":41,"status":"ok","timestamp":1703676211023,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"6Mg7p__ihbOT"},"outputs":[],"source":["'''\n","\n","Here I build templates for the case-of-study Supernova\n","if save == True I'll save out templates + final graph\n","if save == False I don't save anything just use the arrays\n","\n","'''\n","execute = False\n","\n","if execute:\n","  save = True\n","\n","  app_data_path = app_path + new_name + \"/\"\n","  my_class      = build_templates(new_name, app_data_path, bandpass, min_w, max_w, mean_m, mean_q, ref_files, ref_sp, ref_t0)\n","  final_set, final_time_series = my_class.build_and_save(new_name, new_path, save=save)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1703676211023,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"fpYqL1hncEKs","outputId":"5a691749-8e46-4812-c0c8-511baf93f967"},"outputs":[],"source":["'''\n","If I want to analyze every\n","step of the process\n","\n","\n","my_class                           = build_templates(new_name, app_data_path, bandpass, min_w, max_w, mean_m, mean_q, ref_files, ref_sp, ref_t0)\n","lc_filters, lc_set                 = my_class.read_lightcurves()\n","t0                                 = my_class.t0(lc_filters, lc_set)\n","lc_filters, new_set                = my_class.scale_lcs(lc_filters, lc_set, t0)\n","gp_set                             = my_class.gaussian_process_on_lcs(lc_filters, new_set)\n","total_flux, total_wave, total_time = my_class.injection(lc_filters, gp_set)\n","scaled_files, scaled_set           = my_class.scale_real_spectra()\n","under_files                        = my_class.under_files(scaled_files)\n","cut_epoch, cut_points              = my_class.cut_real_spectra(total_flux, scaled_set, under_files)\n","total_flux, total_wave, total_time, add_obs_epoch = my_class.total_spectra(total_flux, total_time, total_wave, under_files,\n","                                                                        scaled_set, cut_epoch, cut_points)\n","new_time_series                    = my_class.get_total_time(add_obs_epoch)\n","gp                                 = my_class.bi_gaussian_process(total_flux, total_wave, total_time) ; print('Check GP')\n","pred_set, new_time_series          = my_class.prediction(gp, new_time_series, total_flux, total_wave, total_time) ; print('Check Prediction')\n","offset                             = my_class.scale_factor()\n","final_set, final_time_series       = my_class.add_offset(offset, pred_set, new_time_series, t0)\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"fMBEJ588xOuu"},"source":["# Section 4: parameters reconstruction"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":369,"status":"ok","timestamp":1703676211383,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"7qgxeLY-pR4E"},"outputs":[],"source":["class parameters():\n","\n","  def __init__(self, name, files, sp_set, sp_filters, lc_filters, lc_set, bandpass, nsteps):\n","\n","    self.name         = name\n","    self.files        = files\n","    self.sp_set       = sp_set\n","    self.lc_filters   = lc_filters\n","    self.lc_set       = lc_set\n","    self.sp_filters   = sp_filters\n","    self.filterlist   = bandpass['filterlist']\n","    self.centroid     = bandpass['centroid']\n","    self.fwhm         = bandpass['fwhm']\n","    self.corr_mag     = bandpass['corr_mag']\n","    self.nsteps       = nsteps\n","\n","  def gaussian_process_on_lcs(self, t0): \n","\n","    gp_set = defaultdict(int)\n","\n","    for filtro in self.lc_filters:\n","      x           = np.array(self.lc_set['time_' + filtro])\n","      y           = np.array(self.lc_set['mag_'  + filtro])\n","      yerr        = np.array(self.lc_set['emag_' + filtro])\n","\n","      x           = np.append(x, t0+200)\n","      y           = np.append(y, 25)\n","      yerr        = np.append(yerr, 0.1*25)\n","\n","      amplitude   = np.mean(y)\n","\n","      lengthscale0 = np.mean(x)\n","      lengthscale1 = np.min(sampling_step(x))\n","      lengthscale2 = np.max(sampling_step(x))\n","\n","      k0           = amplitude * Matern32Kernel(lengthscale0)\n","      k1           = amplitude * Matern32Kernel(lengthscale1)\n","      k2           = amplitude * Matern32Kernel(lengthscale2)\n","\n","      kernel       = k1 + k2 + k0\n","\n","      gp          = george.GP(kernel)\n","      star        = gp.compute(x, yerr)\n","\n","      p0          = gp.get_parameter_vector()\n","      results     = op.minimize(nll, p0, args = (y, gp, star), jac=grad_nll, method=\"L-BFGS-B\")\n","      gp.set_parameter_vector(results.x)\n","\n","      t           = np.linspace(t0, t0+20, len(y))\n","      mu, cov     = gp.predict(y, t)\n","      std         = np.sqrt(cov.diagonal())\n","\n","      gp_set['time_%s'%filtro]   = t\n","      gp_set['mag_%s'%filtro]    = mu\n","      gp_set['emag_%s'%filtro]   = std\n","\n","    return gp_set\n","\n","  def time_of_explosion(self):\n","\n","    t0, t1, dt = ([]), ([]), ([])\n","\n","    for filtro in self.lc_filters:\n","      time = np.array(self.lc_set['time_' + filtro])\n","      t1   = np.append(t1, time[0])\n","      dt   = np.append(dt, avg_distance(time[0:10]))\n","      t0   = np.append(t0, t1-dt)\n","\n","    min_value = np.min(t0)\n","    std_value = np.std(t0)\n","\n","    return min_value, std_value\n","\n","  def extinction(self, gp_set):\n","\n","    if len(set(['B']).intersection(self.lc_filters)) == True: f1 = 'B'\n","    else: f1 = 'g'\n","    if len(set(['V']).intersection(self.lc_filters)) == True: f2 = 'V'\n","    else: f2 = 'i'\n","\n","    timeV    = np.array(gp_set['time_' + f2])\n","    magB     = np.array(gp_set['mag_' + f1])\n","    magV     = np.array(gp_set['mag_' + f2])\n","    emagB    = np.array(gp_set['emag_' + f1])\n","    emagV    = np.array(gp_set['emag_' + f2])\n","\n","    for i in range(len(timeV)):\n","      if magV[i] == np.min(magV):\n","        tmax     = timeV[i]\n","        err_tmax = avg_distance(timeV)\n","\n","    indexB = np.argmin(magB)\n","    indexV = np.argmin(magV)\n","\n","    B  = magB[indexB]\n","    eB = emagB[indexB]\n","    V  = magV[indexV]\n","    eV = emagV[indexV]\n","\n","    E       = (B-V)\n","    sigma_E = np.sqrt(eB**2 + eV**2)\n","\n","    Ax      = 3.1 * E\n","    sigma_A = 3.1 * sigma_E\n","\n","    return Ax, sigma_A, tmax, err_tmax\n","\n","\n","  def velocity(self, save_files, doppler, unc_doppler):\n","\n","    c             = light_vel_km\n","    velocity_set  = defaultdict(int)\n","    velocity_lis  = ([])\n","    unc_velocity  = ([])\n","\n","    for i in range(len(save_files)):\n","\n","      vel     = doppler[i]     * c\n","      unc     = unc_doppler[i] * c\n","\n","      velocity_set['vel_%s'    %str(save_files[i])] = vel\n","      velocity_set['unc_vel_%s'%str(save_files[i])] = unc\n","\n","      velocity_lis = np.append(velocity_lis, vel)\n","      unc_velocity = np.append(unc_velocity, unc)\n","\n","    return velocity_set, velocity_lis, unc_velocity\n","\n","\n","  def med_velocity(self, velocity_lis, unc_velocity):\n","\n","    mean_velocity = np.mean(velocity_lis)\n","    mean_error    = np.sqrt(np.sum(np.square(unc_velocity))) / len(velocity_lis)\n","\n","    return mean_velocity, mean_error\n","\n","  def distance(self, redshift, unc_redshift):\n","\n","    c             = light_vel_km\n","    H             = Hubble_70\n","    distance      = redshift * c / H\n","    unc_dist      = unc_redshift * c / H\n","\n","    return distance, unc_dist\n","\n","\n","  def sed_fitting(self, distance, redshift):\n","\n","    bb_set   = defaultdict(int)\n","    bound_t  = np.array([1000, 30000])\n","    bound_r  = np.array([1000, 20000]) * R_sun_mpc\n","    bound_z  = np.array([0, 1])\n","    bounds   = ([bound_t[0], bound_r[0], bound_z[0]], [bound_t[1], bound_r[1], bound_z[1]])\n","    mh_files = ([])\n","\n","    for epoch in self.files:\n","\n","      wave     = np.array(self.sp_set['wave_'   + str(epoch)])\n","      flux     = np.array(self.sp_set['flux_'   + str(epoch)])\n","\n","      sp_bands = self.sp_filters['filt_' + str(epoch)]\n","      sed      = np.array([])\n","      wave_c   = np.array([])\n","\n","      for filtro in sp_bands:\n","\n","        min_wave = [self.centroid[i]-self.fwhm[i]/2   for i in range(0, len(self.filterlist))  if self.filterlist[i] == filtro]\n","        max_wave = [self.centroid[i]+self.fwhm[i]/2   for i in range(0, len(self.filterlist))  if self.filterlist[i] == filtro]\n","        med_wave = [self.centroid[i]                  for i in range(0, len(self.filterlist))  if self.filterlist[i] == filtro]\n","\n","        mask     = np.logical_and(wave >= min_wave, wave<= max_wave)\n","        int_flux = trapz( flux[mask] , wave[mask])\n","\n","        if int_flux > 0:\n","          sed     = np.append(sed, int_flux)\n","          wave_c  = np.append(wave_c, med_wave[0])\n","\n","      if len(sed) >=5:\n","\n","        wave_c, sed = time_sorting(wave_c, sed)\n","        wave_c      = wave_c/(1+redshift)\n","\n","        err         = np.linspace(np.std(sed), np.std(sed), len(sed))\n","\n","        fit_func = partial(blackbody, distance=distance)\n","        pars, cov = curve_fit(fit_func, wave_c , sed, sigma = err, maxfev=50000, bounds=bounds)\n","\n","        if np.any(np.isinf(cov)) == False:\n","          bb_set['temperature_%s'%epoch]   = pars[0]\n","          bb_set['radius_%s'%epoch]        = pars[1]\n","          bb_set['zeta_%s'%epoch]          = pars[2]\n","          bb_set['cov_%s'%epoch]           = cov\n","          bb_set['wavec_%s'%epoch]         = wave_c\n","          bb_set['sed_%s'  %epoch]         = sed\n","          bb_set['err_%s'  %epoch]         = err\n","\n","          mh_files = np.append(mh_files, epoch)\n","\n","    MH                                        = hastings(self.nsteps, bounds, model = fit_func)\n","    mh_set                                    = MH.final_results_sedfitting(mh_files, self.sp_set, bb_set)\n","\n","    return (mh_set, mh_files)\n","\n","  def med_sed(self, mh_set, mh_files):\n","\n","    tem, rad, zet       = ([]), ([]), ([])\n","    err_t, err_r, err_z = ([]), ([]), ([])\n","    for epoch in mh_files:\n","      tem = np.append(tem, mh_set['temperature_' + str(epoch)])\n","      rad = np.append(rad, mh_set['radius_' + str(epoch)])\n","      zet = np.append(zet, mh_set['zeta_' + str(epoch)])\n","\n","      err_t = np.append(err_t, mh_set['err_t_' + str(epoch)])\n","      err_r = np.append(err_r, mh_set['err_r_' + str(epoch)])\n","      err_z = np.append(err_z, mh_set['err_z_' + str(epoch)])\n","\n","    temperature   = np.mean(tem)\n","    radius        = np.mean(rad)\n","    zeta          = np.mean(zet)\n","    mean_error_t  = np.sqrt(np.sum(np.square(err_t))) / len(tem)\n","    mean_error_r  = np.sqrt(np.sum(np.square(err_r))) / len(rad)\n","    mean_error_z  = np.sqrt(np.sum(np.square(err_z))) / len(zet)\n","\n","    return temperature, radius, zeta, mean_error_t, mean_error_r, mean_error_z\n","\n","\n","  def prog_radius(self, mh_set, t0, mh_files):\n","\n","    radius  = ([])\n","    time    = ([])\n","\n","    for epoch in mh_files:\n","      if epoch - int(t0) < 60:\n","\n","        radius = np.append(radius, mh_set['radius_' + str(epoch)])\n","        time   = np.append(time, epoch-int(t0))\n","\n","    radius      = radius * 3.08567758128e+19  / R_sun_km\n","    mask        = (time >= 0) & (time <= 20)\n","    mask_time   = time[mask]\n","    mask_radius = radius[mask]\n","\n","    x      = mask_time\n","    y      = mask_radius\n","    yerr   = y*0.1\n","\n","    amplitude   = np.mean(y)\n","\n","    lengthscale0 = np.mean(x)\n","    lengthscale1 = np.min(sampling_step(x))\n","    lengthscale2 = np.max(sampling_step(x))\n","\n","    k0           = amplitude * Matern32Kernel(lengthscale0)\n","    k1           = amplitude * Matern32Kernel(lengthscale1)\n","    k2           = amplitude * Matern32Kernel(lengthscale2)\n","\n","    kernel       = k1 + k2 + k0\n","\n","    gp          = george.GP(kernel)\n","    star        = gp.compute(x, yerr)\n","\n","    p0          = gp.get_parameter_vector()\n","    results     = op.minimize(nll, p0, args = (y, gp, star), jac=grad_nll, method=\"L-BFGS-B\")\n","    gp.set_parameter_vector(results.x)\n","\n","    t           = np.linspace(0, np.max(x), len(x))\n","    mu, cov     = gp.predict(y, t)\n","    std         = np.sqrt(cov.diagonal())\n","    R0     = mu[0]\n","    err_R0 = std[0]\n","\n","    if int(time[0]) == 0 and radius[0] < mu[0]:\n","      R0 = radius[0]\n","\n","    return R0, err_R0\n","\n","\n","  def luminosity(self, tmax, distance, inc_d, gp_set):\n","\n","    peak_flux  = ([])\n","    peak_wave  = ([])\n","    peak_error_flux = ([])\n","\n","    for filtro in lc_filters:\n","      #if filtro != 'UVW2' and filtro !='UVM2' and filtro !='UVW1':\n","        t            = lc_set['time_' + filtro]\n","        close_time   = min(t, key = lambda x: abs(x-tmax))\n","        if close_time - tmax < 10:\n","          mag       = gp_set['mag_' + filtro]\n","          time      = gp_set['time_' + filtro]\n","          emag      = gp_set['emag_' + filtro]\n","\n","          closest   = min(time, key = lambda x: abs(x-tmax))\n","          peak_mag  = [mag[i]           for i in range(len(mag))              if time[i] == closest]\n","          peak_emag = [emag[i]          for i in range(len(mag))              if time[i] == closest]\n","          wavv      = [self.centroid[i] for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","\n","          single_flux = light_vel_A * 10**(- 0.4* (peak_mag[0] + 48.6)) / wavv[0] **2\n","          peak_flux = np.append(peak_flux, single_flux)\n","\n","          peak_wave = np.append(peak_wave, wavv[0])\n","\n","          single_flux_unc = abs(-0.4 * math.log(10) * single_flux)*peak_emag[0]\n","          peak_error_flux = np.append(peak_error_flux, single_flux_unc)\n","\n","    peak_wave, peak_flux, peak_error_flux = order_sorting(peak_wave, peak_flux, peak_error_flux)\n","\n","    int_flux        = trapz(peak_flux, peak_wave)                                  #erg/s/cm^2\n","    delta_int_flux = 0.0\n","\n","    for i in range(len(peak_flux)):\n","        derivative_flux = peak_wave[i]\n","        derivative_wave = peak_flux[i]\n","        delta_int_flux += (derivative_flux * peak_error_flux[i])**2 + (derivative_wave * peak_wave[i])**2\n","\n","    delta_int_flux = np.sqrt(delta_int_flux)\n","\n","    dist_cm  = distance * 3.0856775813e+24\n","    inc_d_cm = inc_d    * 3.0856775813e+24\n","\n","    Lum  = 4*np.pi * dist_cm**2 * int_flux                                   #erg/s\n","\n","    der_dist = 8*np.pi*dist_cm*int_flux\n","    der_flux = 4*np.pi*dist_cm**2\n","\n","    eLum = np.sqrt( (der_dist*inc_d_cm)**2 + (der_flux*delta_int_flux))\n","\n","    return Lum, eLum\n","\n","\n","  def energy(self, tmax, Lum, error_Lum):\n","\n","    tmax_s     = tmax * 86400\n","    energy     = Lum * tmax_s       #erg\n","    err_energy = error_Lum * tmax_s\n","    \n","    return energy, err_energy\n","\n","\n","  def ejecta_mass(self, tmax, save_files,velocity_set, energy, error_energy):\n","\n","    epoch   = min(save_files, key = lambda x: abs(x-tmax))\n","    vel     = velocity_set['vel_' + str(epoch)]         #km/s\n","    err_v   = velocity_set['unc_vel_%s'%str(epoch)]     #km/s\n","    vel_cm  = vel   * 100000                            #cm/s\n","    err_v   = err_v * 100000\n","\n","    Mej     = 6/5 * energy / vel_cm**2\n","\n","    der_eg  = 6/5 / vel_cm**2\n","    der_vel = 12/5 * energy/vel_cm**3\n","\n","    err_Mej = np.sqrt( (der_eg*error_energy)**2 + (der_vel*err_v)**2)\n","\n","    return Mej/M_sun_g, err_Mej/M_sun_g\n","\n","  def prog_mass(self, Mej):\n","\n","    M_NS = 1.2 #solar mass\n","    M_BH = 10  #solar mass\n","\n","    Mpr  = (Mej + M_NS, Mej + M_BH)\n","\n","    return Mpr\n","\n","  def luminosity_vs_time(self, t0, distance, decay_time):\n","\n","    dist_cm    = distance * 3.0856775813e+24\n","    lum_array  = ([])\n","    time_array = ([])\n","\n","    for t1 in decay_time:\n","      flux = ([])\n","      wave = ([])\n","      for filtro in self.lc_filters:\n","        mag       = self.lc_set['mag_' + filtro]\n","        time      = self.lc_set['time_' + filtro]\n","        closest   = min(time, key = lambda x: abs(x-t1))\n","        if abs(t1-closest) < 5:\n","          magg      = [mag[i]           for i in range(len(mag))              if time[i] == closest]\n","          wavv      = [self.centroid[i] for i in range(len(self.filterlist))  if self.filterlist[i] == filtro]\n","          flux      = np.append(flux, light_vel_A * 10**(- 0.4* (magg[0] + 48.6)) / wavv[0] **2)\n","          wave      = np.append(wave, wavv[0])\n","        else:\n","          continue\n","\n","      if len(wave) < 1: continue\n","\n","      wave, flux = time_sorting(wave, flux)\n","      int_flux   = trapz(flux, wave)                             #erg/s/cm^2\n","      Lum        = 4*np.pi * dist_cm**2 * int_flux               #erg/s\n","\n","      lum_array  = np.append(lum_array, Lum)\n","      time_array = np.append(time_array, t1-t0)\n","\n","    return lum_array, time_array\n","\n","  def nichel_mass(self, distance, t0):\n","\n","    t1 = t0+120\n","    t2 = t0+300\n","    lum_array       = np.zeros(1)\n","    time_array      = ([])\n","    i               = 0\n","\n","    redux_step = 5\n","\n","    while len(set(lum_array)) - list(lum_array).count(0) < 4:\n","\n","      t1                    = t0 + 120 - redux_step*i\n","      decay_time            = np.arange(t1, t2, dtype=int, step = 5)\n","      lum_array, time_array = self.luminosity_vs_time(t0, distance, decay_time)\n","      i+=1\n","\n","      if t1-t0<0: break\n","\n","    gamma_1 = 1.32e-6 #s-1  Ni decay rate\n","    gamma_2 = 1.02e-7 #s-1  Co decay rate\n","\n","    t       = time_array * 86400\n","    x       = 3.90e+10 * np.exp(-gamma_1*t) + 6.78e+9 * (np.exp(-gamma_2*t) - np.exp(-gamma_1*t))\n","    y       = lum_array\n","    linear_model  = np.polyfit(x,y,1)\n","    m       = linear_model[0]\n","    q       = linear_model[1]\n","    Mni     = m/M_sun_g\n","\n","    residuals               = y - (m * x + q)\n","    residual_sum_of_squares = np.sum(residuals**2)\n","    std_error_m             = np.sqrt(residual_sum_of_squares / (len(x) - 2)) / np.sqrt(np.sum((x - np.mean(x))**2))\n","    err_Mni                 = std_error_m/M_sun_g\n","\n","    return Mni, err_Mni\n","\n","  def run_parameters(self):\n","\n","    t0, err_t0                                         = self.time_of_explosion()\n","    gp_set                                             = self.gaussian_process_on_lcs(t0)\n","    Ax, err_Ax, tmax, err_tmax                         = self.extinction(gp_set)\n","    save_files, doppler, err_dop, redshift, err_red, classe = line_fitting(self.files, self.sp_set, t0).velocity_and_redshift()\n","    velocity_set, velocity_lis, err_velocity           = self.velocity(save_files, doppler, err_dop)\n","    mean_vel, mean_err_vel                             = self.med_velocity(velocity_lis, err_velocity)\n","    dist, err_d                                        = self.distance(redshift, err_red)\n","    mh_set, mh_files                                   = self.sed_fitting(dist, redshift)\n","    tem, rad, zet, mean_err_t, mean_err_r, mean_err_z  = self.med_sed(mh_set, mh_files)\n","    R, err_R                                           = self.prog_radius(mh_set, t0, mh_files)\n","    Lum, err_L                                         = self.luminosity(tmax, dist, err_d, gp_set)\n","    energy, err_E                                      = self.energy(tmax, Lum, err_L)\n","    Mej, err_Mej                                       = self.ejecta_mass(tmax, save_files, velocity_set, energy, err_E)\n","    Mni, err_Mni                                       = self.nichel_mass(dist, t0)\n","    Mpr                                                = self.prog_mass(Mej)\n","\n","    results = {\n","        'classe'        : classe,\n","        'extinction'    : (round(Ax,2) , round(err_Ax, 2)),\n","        't0'            : (round(t0, 2), round(err_t0, 2)),\n","        'tmax'          : (round(tmax, 2), round(err_tmax, 2)),\n","        'vel'           : (int(mean_vel), int(mean_err_vel)),\n","        'red'           : (round(redshift, 4), round(err_red, 4)),\n","        'dist'          : (round(dist, 2), round(err_d, 2)),\n","        'temp'          : (int(tem), int(mean_err_t)),\n","        'rad'           : (int(rad/R_sun_mpc), int(mean_err_r/R_sun_mpc)),\n","        'zeta'          : (round(zet, 3), round(mean_err_z, 3)),\n","        'Lum'           : (round(Lum/10**(41),3), round(err_L/10**(41), 3)),\n","        'E'             : (round(energy/10**(52), 3), round(err_E/10**(52), 3)),\n","        'Mej'           : (round(Mej, 3), round(err_Mej, 3)),\n","        'Mni'           : (round(Mni, 5), round(err_Mni, 5)),\n","        'Mpr'           : (Mpr, round(err_Mej, 3)),\n","        'prad'          : (int(R), int(err_R))\n","\n","    }\n","\n","    return (results)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1703676211383,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"nE4Gpbo5Snsv"},"outputs":[],"source":["class line_fitting():\n","\n","  def __init__(self, files, temp_set, t0):\n","\n","    self.files    = files\n","    self.temp_set = temp_set\n","    self.t0       = t0\n","\n","  def starting_epoch(self):\n","\n","    YES = False\n","    i   = 0\n","    while YES == False:\n","\n","      epoch = self.files[i]\n","      wave  = np.array(self.temp_set['wave_'  + str(epoch)])\n","      flux  = np.array(self.temp_set['flux_'  + str(epoch)])\n","      plt.figure(figsize=(10,7))\n","      for line, value in lines.items():\n","        if value < np.max(wave) and value > np.min(wave):\n","          plt.plot(wave, flux, '.', color='grey')\n","          plt.axvline(value, 0, 1,  label=line)\n","          plt.legend()\n","      plt.show()\n","      print('Can you see at least one P-Cygni and one emission line?')\n","      answer = input().lower()\n","      if answer == 'yes':\n","        YES = True\n","      elif answer == 'no':\n","        YES = False\n","        i+=1\n","      else:\n","        print(\"Please enter only 'yes' or 'no'\")\n","\n","    return epoch\n","\n","  def find_lines(self, epoch):\n","\n","    wave  = np.array(self.temp_set['wave_'  + str(epoch)])\n","    flux  = np.array(self.temp_set['flux_'  + str(epoch)])\n","    print('Select the P-Cygni and the emission lines')\n","    print('The answer should be comma-space separated i.e. He Ia, Ha')\n","    for line, value in lines.items():\n","      if value < np.max(wave) and value > np.min(wave):\n","        plt.plot(wave, flux, '-')\n","        plt.axvline(value, 0, 1, label=line)\n","        plt.legend()\n","        plt.show()\n","\n","    OK = False \n","    while OK == False: \n","      answer        = input().split(', ')\n","      if answer[0] in lines.keys() and answer[1] in lines.keys():\n","        p_cygni_line  = answer[0]\n","        emission_line = answer[1]\n","        OK  = True\n","      else: \n","        print('Please select an existing line')\n","        OK = False \n","\n","    return p_cygni_line, emission_line\n","  \n","\n","  def p_cygni_interval(self, epoch, p_cygni_line):\n","\n","    wave  = np.array(self.temp_set['wave_'  + str(epoch)])\n","    flux  = np.array(self.temp_set['flux_'  + str(epoch)])\n","\n","    red, blu = 50, 250  #this are just hypotetical values\n","\n","    stop = False\n","\n","    while stop == False:\n","\n","      for line, value in lines.items():\n","        if line == p_cygni_line:\n","          blue_limit        = value - blu\n","          red_limit         = value + red\n","          mask              = np.logical_and(wave >= blue_limit, wave <= red_limit)\n","          in_wave, in_flux  = wave[mask], flux[mask]\n","\n","          plt.plot(in_wave, in_flux)\n","          plt.axvline(value, 0, 1)\n","          plt.show()\n","          print('Please insert values to enlarge or reduce the interval i.e. \"10, -35\"')\n","          print('The first value will add on the left, while the second on the right')\n","          print('If the interval is okay, just write 0, 0')\n","          answer     = input()\n","          values     = answer.split(', ')\n","          blue_value = int(values[0])\n","          red_value  = int(values[1])\n","\n","          red += red_value\n","          blu += blue_value\n","          if red_value == blue_value == 0.0:\n","            stop = True\n","          else:\n","            stop = False\n","\n","    return blu, red\n","\n","  def emission_interval(self, epoch, emission_line):\n","\n","    wave  = np.array(self.temp_set['wave_'  + str(epoch)])\n","    flux  = np.array(self.temp_set['flux_'  + str(epoch)])\n","\n","    red_e, blu_e = 50, 250  #this are just hypotetical values to start with\n","\n","    stop = False\n","\n","    while stop == False:\n","\n","      for line, value in lines.items():\n","        if line == emission_line:\n","          blue_limit        = value - blu_e\n","          red_limit         = value + red_e\n","          mask              = np.logical_and(wave >= blue_limit, wave <= red_limit)\n","          in_wave, in_flux  = wave[mask], flux[mask]\n","\n","          plt.plot(in_wave, in_flux)\n","          plt.axvline(value, 0, 1)\n","          plt.show()\n","          print('Please insert values to enlarge or reduce the interval i.e. \"10, -35\"')\n","          print('The first value will add on the left, while the second on the right')\n","          print('If the interval is okay, just write 0, 0')\n","          answer     = input()\n","          values     = answer.split(', ')\n","          blue_value = int(values[0])\n","          red_value  = int(values[1])\n","\n","          red_e += red_value\n","          blu_e += blue_value\n","          if red_value == blue_value == 0.0:\n","            stop = True\n","          else:\n","            stop = False\n","\n","    return blu_e, red_e\n","\n","\n","  def p_cygni_fitter(self, p_cygni_line, blu, red):\n","\n","    save_files  = []\n","    doppler     = ([])\n","    err_dop     = ([])\n","    stop        = 0\n","\n","    for epoch in self.files:\n","      wave  = np.array(self.temp_set['wave_'  + str(epoch)])\n","      flux  = np.array(self.temp_set['flux_'  + str(epoch)])\n","\n","      for line, value in lines.items():\n","        if line == p_cygni_line:\n","          blue_limit = value - blu\n","          red_limit  = value + red\n","          mask    = np.logical_and(wave >= blue_limit, wave <= red_limit)\n","          in_wave = wave[mask]\n","          in_flux = flux[mask]\n","          if len(in_flux) > 10:\n","            my_fitting  = np.polyfit(in_wave, in_flux, 4, full=True)\n","            SSE         = my_fitting[1][0]\n","            SST         = np.sum((in_flux - in_flux.mean())**2)\n","            R2          =  1 - SSE/SST\n","            if R2 > 0.9:\n","              warnings.filterwarnings(\"ignore\")\n","              pars        = np.poly1d(np.polyfit(in_wave, in_flux, 8))\n","              maxi        = in_wave[np.argmax(pars(in_wave))]\n","              mini        = in_wave[np.argmin(pars(in_wave))]\n","              if maxi > mini:\n","                if abs(maxi-np.max(in_wave)) != 0:\n","\n","                  obs             = (maxi + mini) / 2\n","                  shift_doppler   = abs(obs-value)  / value\n","                  doppler         = np.append(doppler, shift_doppler)\n","\n","                  err_w           = avg_distance(in_wave)\n","                  err_obs         = 1/np.sqrt(2) * err_w\n","                  err_dop         = np.append(err_dop, err_obs / value)\n","\n","\n","                  save_files.append(epoch)\n","                  stop  = 0\n","\n","                else:\n","                  stop = 1\n","                  break\n","\n","      if stop == 1:\n","        break\n","\n","    return (save_files, doppler, err_dop)\n","\n","\n","  def emission_fitter(self, epoch, emission_line, blu_e, red_e):\n","\n","\n","    wave       = np.array(self.temp_set['wave_'  + str(epoch)])\n","    flux       = np.array(self.temp_set['flux_'  + str(epoch)])\n","\n","    for line, value in lines.items():\n","      if line == emission_line:\n","        blue_limit = value - blu_e\n","        red_limit  = value + red_e\n","        mask    = np.logical_and(wave >= blue_limit, wave <= red_limit)\n","        in_wave = wave[mask]\n","        in_flux = flux[mask]\n","\n","        if len(in_flux) > 10:\n","          my_fitting  = np.polyfit(in_wave, in_flux, 4, full=True)\n","          SSE         = my_fitting[1][0]\n","          SST         = np.sum((in_flux - in_flux.mean())**2)\n","          R2          =  1 - SSE/SST\n","          if R2 > 0.9:\n","            warnings.filterwarnings(\"ignore\")\n","            pars        = np.poly1d(np.polyfit(in_wave, in_flux, 8))\n","            maxi        = in_wave[np.argmax(pars(in_wave))]\n","            mini        = in_wave[np.argmin(pars(in_wave))]\n","            if maxi > mini:\n","                if abs(maxi-np.max(in_wave)) != 0:\n","\n","                  obs         = maxi\n","                  shift_cosmo = (obs-value) / value\n","                  redshift    = shift_cosmo\n","\n","                  err_obs     = avg_distance(in_wave)\n","                  err_red     = err_obs/value\n","\n","    return redshift, err_red\n","\n","\n","  def classification(self, p_cygni_line):\n","\n","    Ib = ['He Ia','He Ib', 'He Ic', 'He Id', 'He Ie' ]\n","    II = ['Ha', 'Hb', 'Hg', 'Hd']\n","\n","    classe = []\n","\n","    if p_cygni_line in II:\n","      classe = 'II'\n","    elif p_cygni_line in Ib:\n","      classe = 'Ib'\n","    else:\n","      classe = 'Ic'\n","\n","    return classe\n","\n","\n","  def velocity_and_redshift(self):\n","\n","    epoch                          = self.starting_epoch()\n","    p_cygni_line, emission_line    = self.find_lines(epoch)\n","    blu, red                       = self.p_cygni_interval(epoch, p_cygni_line)\n","    blu_e, red_e                   = self.emission_interval(epoch, emission_line)\n","    save_files, doppler, err_dop   = self.p_cygni_fitter(p_cygni_line, blu, red)\n","    redshift, err_red              = self.emission_fitter(epoch, emission_line, blu_e, red_e)\n","    classe                         = self.classification(p_cygni_line)\n","\n","    return save_files, doppler, err_dop, redshift, err_red, classe\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1703676211384,"user":{"displayName":"andrea simongini","userId":"11731786354185948638"},"user_tz":-60},"id":"RI_ZRAhIpXCL"},"outputs":[],"source":["class hastings:\n","\n","  def __init__(self, nsteps, bounds, model):\n","    self.nsteps     = nsteps\n","    self.bounds     = bounds\n","    self.model      = model\n","\n","  def get_model_predictions(self, theta, x):\n","    return self.model(x, theta[0], theta[1], theta[2])\n","\n","  def lnlikelihood(self, theta, x, y, err):\n","    model_preds = self.get_model_predictions(theta, x)\n","    lnl         = -np.sum((y - model_preds) ** 2 / (2 * err ** 2))\n","    return lnl\n","\n","  def lnprior(self, theta):\n","    a, b, c = theta\n","    a0, a1 = self.bounds[0][0], self.bounds[1][0]\n","    b0, b1 = self.bounds[0][1], self.bounds[1][1]\n","    c0, c1 = self.bounds[0][2], self.bounds[1][2]\n","    if a0 <= a <= a1 and b0 <= b <= b1 and c0 <= c <= c1:\n","      lnp = np.log(1/(a1-a0)) + np.log(1/(b1-b0)) + np.log(1/(c1-c0))\n","    else:\n","      return -np.inf\n","    return lnp\n","\n","  def lnposterior(self, theta, x, y, err):\n","    lnp = self.lnprior(theta)\n","    if not np.isfinite(lnp):\n","        return -np.inf\n","    lnl = self.lnlikelihood(theta, x, y, err)\n","    lnpost = lnl + lnp\n","    return lnpost\n","\n","  def hastings_ratio(self, theta_1, theta_0, x, y, err):\n","    lnpost1 = self.lnposterior(theta_1, x, y, err)\n","    lnpost0 = self.lnposterior(theta_0, x, y, err)\n","\n","    expost1 = np.exp(lnpost1)\n","    expost0 = np.exp(lnpost0)\n","\n","    if expost1 != 0 and expost0 != 0:\n","      h_ratio = expost1 / expost0\n","      return h_ratio\n","    else:\n","      return 0\n","\n","  def propose_jump(self, theta, cov):\n","\n","    if np.shape(theta) == np.shape(cov):\n","        cov = np.diag(np.array(cov)**2)\n","    proposed_position = np.random.multivariate_normal(theta, cov)\n","\n","    return proposed_position\n","\n","  def mh_mcmc(self, theta0, x, y, err, cov):\n","\n","    positions         = np.zeros((self.nsteps+1, len(theta0)))\n","    lnpost_at_pos     = -np.inf*np.ones(self.nsteps+1)\n","    acceptance_ratio  = np.zeros_like(lnpost_at_pos)\n","    accepted          = 0\n","\n","    positions[0]      = theta0\n","    lnpost_at_pos[0]  = self.lnposterior(theta0, x, y, err)\n","\n","    for step_num in np.arange(1, self.nsteps+1):\n","        proposal  = self.propose_jump(positions[step_num-1], cov)\n","        H         = self.hastings_ratio(proposal, positions[step_num-1], x, y, err)\n","        R         = np.random.uniform()\n","\n","        if H > R:\n","            accepted += 1\n","            positions[step_num]       = proposal\n","            lnpost_at_pos[step_num]   = self.lnposterior(proposal, x, y, err)\n","            acceptance_ratio[step_num]= float(accepted)/step_num\n","        else:\n","            positions[step_num]       = positions[step_num-1]\n","            lnpost_at_pos[step_num]   = lnpost_at_pos[step_num-1]\n","            acceptance_ratio[step_num]= float(accepted)/step_num\n","\n","    return (positions, lnpost_at_pos, acceptance_ratio)\n","\n","  def final_results_sedfitting(self, files, sp_set, bb_set):\n","\n","    mh_set = defaultdict(int)\n","    ex     = []\n","    for epoch in files:\n","\n","      T0   = np.array(bb_set['temperature_' + str(epoch)])\n","      R0   = np.array(bb_set['radius_'      + str(epoch)])\n","      Z0   = np.array(bb_set['zeta_'        + str(epoch)])\n","      cov  = np.array(bb_set['cov_'         + str(epoch)])\n","      x    = np.array(bb_set['wavec_'       + str(epoch)])\n","      y    = np.array(bb_set['sed_'         + str(epoch)])\n","      err  = np.array(bb_set['err_'         + str(epoch)])\n","      ex.append(epoch)\n","\n","      theta0         = (T0, R0, Z0)\n","      pos, lnpost, _ = self.mh_mcmc(theta0, x, y, err, cov)\n","      for i in range(len(pos)):\n","        if lnpost[i] == np.max(lnpost):\n","          pars = pos[i]\n","\n","          final_t = pars[0] * np.sqrt(pars[2])\n","          final_r = pars[1]\n","          final_z = pars[2]\n","          err_t   = (np.sqrt(final_z) * final_t * 0.1) + 1/2*final_t* np.sqrt(final_z) * 0.1\n","          err_r   = final_r * 0.1\n","          err_z   = final_z * 0.1\n","\n","          mh_set['temperature_' + str(epoch)]  = final_t\n","          mh_set['radius_' + str(epoch)]       = final_r\n","          mh_set['zeta_' + str(epoch)]         = final_z\n","          mh_set['err_t_' + str(epoch)]        = err_t\n","          mh_set['err_r_' + str(epoch)]        = err_r\n","          mh_set['err_z_' + str(epoch)]        = err_z\n","\n","    return mh_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxburXmQkJDD"},"outputs":[],"source":["'''\n","Now I apply the parameter reconstruction on my case-of-study SN\n","'''\n","\n","execute = True\n","\n","if execute:\n","\n","  lc_filters, lc_set          = read_lightcurves(new_name, new_path)\n","  files, temp_set, sp_filters = templates_extraction(new_name, new_path, acc, tresh, bandpass).use_templates()\n","  par                         = parameters(new_name, files, temp_set, sp_filters, lc_filters, lc_set, bandpass, nsteps)\n","  results                     = par.run_parameters()\n","  print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZcAVdw22Ckn"},"outputs":[],"source":["'''\n","If you want to analyse single parameters\n","'''\n","\n","t0, err_t0                                         = par.time_of_explosion()\n","gp_set                                             = par.gaussian_process_on_lcs(t0)\n","Ax, err_Ax, tmax, err_tmax                         = par.extinction(gp_set)\n","save_files, doppler, err_dop, redshift, err_red, classe = line_fitting(files, temp_set, t0).velocity_and_redshift()\n","velocity_set, velocity_lis, err_velocity           = par.velocity(save_files, doppler, err_dop)\n","mean_vel, mean_err_vel                             = par.med_velocity(velocity_lis, err_velocity)\n","dist, err_d                                        = par.distance(redshift, err_red)\n","mh_set, mh_files                                   = par.sed_fitting(dist, redshift)\n","tem, rad, zet, mean_err_t, mean_err_r, mean_err_z  = par.med_sed(mh_set, mh_files)\n","R, err_R                                           = par.prog_radius(mh_set, t0, mh_files)\n","Lum, err_L                                         = par.luminosity(tmax, dist, err_d, gp_set)\n","energy, err_E                                      = par.energy(tmax, Lum, err_L)\n","Mej, err_Mej                                       = par.ejecta_mass(tmax, save_files, velocity_set, energy, err_E)\n","Mni, err_Mni                                       = par.nichel_mass(dist, t0)\n","Mpr                                                = par.prog_mass(Mej)\n","\n","print('class:', '\\t', classe)\n","print('Ax:', '\\t', round(Ax,2), '+/-', round(err_Ax, 2))\n","print('t0:', '\\t', round(t0, 2), '+/-', round(err_t0, 2))\n","print('tmax:', '\\t', round(tmax, 2), '+/-',  round(err_tmax, 2))\n","print('vel:', '\\t', int(mean_vel), '+/-', int(mean_err_vel))\n","print('z:', '\\t', round(redshift, 4), '+/-', round(err_red, 4))\n","print('dist:', '\\t',  round(dist, 2), '+/-', round(err_d, 2))\n","print('tem:', '\\t', int(tem), '+/-', int(mean_err_t))\n","print('rad:', '\\t', int(rad/R_sun_mpc), '+/-', int(mean_err_r/R_sun_mpc))\n","print('zeta:', '\\t', round(zet, 3), '+/-', round(mean_err_z, 3))\n","print('L:', '\\t', round(Lum/10**(41),3), '+/-', round(err_L/10**(41), 3))\n","print('E:', '\\t', round(energy/10**(52), 3), '+/-', round(err_E/10**(52), 3))\n","print('Mej:', '\\t', round(Mej, 3), '+/-', round(err_Mej, 3))\n","print('Mni:', '\\t', round(Mni, 5), '+/-', round(err_Mni, 5))\n","print('Mpr:', '\\t', '(', Mpr[0], ',', Mpr[1], ')', '+/-', round(err_Mej, 3))\n","print('R:', '\\t', int(R), '+/-', int(err_R))\n","\n"]}],"metadata":{"colab":{"collapsed_sections":["GoiUwFhRD-5i","8R-8O1XBj94T","w23DZ4yta6c_","Kny8mlMeGrOa","8B-Ho2A44zDA","BgdpLgnyLbdH","IQ1a_mcXZpnH"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
